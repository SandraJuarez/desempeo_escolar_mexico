{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clean_database\n",
    "import DataAugmentation\n",
    "import Feature_Engineering as fe\n",
    "import train_test_validation as ttv\n",
    "import graficar\n",
    "import linear\n",
    "from linear import Linear_Model\n",
    "import logistic\n",
    "import mixture\n",
    "\n",
    "#from mlp import MultilayerPerceptron\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import y_hot as hot\n",
    "from mlp_mlflow import MultilayerPerceptron\n",
    "import mlflow\n",
    "import importlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limpieza de la base de datos\n",
    "====================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3364 7882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\52333\\Documents\\doctorado\\ml-md\\proyecto\\clean_database.py:375: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfd=d1.append(d2)\n"
     ]
    }
   ],
   "source": [
    "datos_bajos,datos=clean_database.Clean_Data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data augmentation para la clase de datos '1' (alumnos con bajo desempeño escolar)\n",
    "====================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3364, 96)\n"
     ]
    }
   ],
   "source": [
    "datos_aumentados=DataAugmentation.Smote(datos_bajos)\n",
    "print(datos_aumentados.shape)\n",
    "datos=np.concatenate((datos,datos_aumentados),axis=0)\n",
    "#datos=datos[np.random.permutation(datos.shape[0]),:]\n",
    "x,y=datos[:,1:],datos[:,0]\n",
    "np.savetxt('datosy.csv',datos,delimiter=',')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering (Backwards selection)\n",
    "==============================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estas son las muestras en casda clase [[0.35       0.25       0.33333333 ... 0.4040404  1.         0.66666667]\n",
      " [0.15       0.16666667 0.08333333 ... 0.         0.         0.66666667]\n",
      " [0.1        0.         0.16666667 ... 0.4040404  0.375      0.83333333]\n",
      " ...\n",
      " [0.21052632 0.24677825 0.08655508 ... 0.455717   0.2693305  0.67311017]\n",
      " [0.25268668 0.23342058 0.16666667 ... 0.48484848 0.67473825 0.66666667]\n",
      " [0.18602135 0.12786714 0.16666667 ... 0.44166418 0.82540214 0.66666667]]\n",
      "Los índices ganadores fueron: (4, 5, 14, 15, 16, 17, 18, 19, 23, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 43, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 91, 92, 93)\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(fe)\n",
    "x1,x2=fe.separate(x,y) #tenemos que separar los features en\n",
    "size=x1.shape[1]\n",
    "min_group=70 #son los features con los que nos queremos quedar\n",
    "x1,x2,ganadores=fe.sequential_backwards(size,x1,x2,min_group)\n",
    "x=x[:,ganadores]\n",
    "x_train, x_test, x_v,y_train, y_test, y_v=ttv.split_data(x,y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multilayer Perceptron\n",
    "======================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING: In the epoch     0 the loss is 2.11892 and the accuracy is 0.42861\n",
      "VALIDATION: In the epoch     0 the loss is 2.04970 and the accuracy is 0.41881\n",
      "TRAINING:In the epoch     0 the precission is 0.23214 and the recall is 0.22763\n",
      "VALIDATION:In the epoch     0 the precission is 0.32345 and the recall is 0.29692\n",
      "TRAINING: In the epoch    10 the loss is 1.60480 and the accuracy is 0.49982\n",
      "VALIDATION: In the epoch    10 the loss is 1.56820 and the accuracy is 0.49684\n",
      "TRAINING: In the epoch    20 the loss is 1.31062 and the accuracy is 0.53490\n",
      "VALIDATION: In the epoch    20 the loss is 1.29404 and the accuracy is 0.52847\n",
      "TRAINING: In the epoch    30 the loss is 1.11625 and the accuracy is 0.55325\n",
      "VALIDATION: In the epoch    30 the loss is 1.10935 and the accuracy is 0.55377\n",
      "TRAINING: In the epoch    40 the loss is 0.97168 and the accuracy is 0.56598\n",
      "VALIDATION: In the epoch    40 the loss is 0.96997 and the accuracy is 0.56263\n",
      "TRAINING: In the epoch    50 the loss is 0.85598 and the accuracy is 0.58025\n",
      "VALIDATION: In the epoch    50 the loss is 0.85716 and the accuracy is 0.57866\n",
      "TRAINING: In the epoch    60 the loss is 0.76228 and the accuracy is 0.59445\n",
      "VALIDATION: In the epoch    60 the loss is 0.76517 and the accuracy is 0.59216\n",
      "TRAINING: In the epoch    70 the loss is 0.68702 and the accuracy is 0.60598\n",
      "VALIDATION: In the epoch    70 the loss is 0.69179 and the accuracy is 0.60439\n",
      "TRAINING: In the epoch    80 the loss is 0.62726 and the accuracy is 0.61779\n",
      "VALIDATION: In the epoch    80 the loss is 0.63262 and the accuracy is 0.61620\n",
      "TRAINING: In the epoch    90 the loss is 0.57905 and the accuracy is 0.62854\n",
      "VALIDATION: In the epoch    90 the loss is 0.58440 and the accuracy is 0.62716\n",
      "TRAINING: In the epoch   100 the loss is 0.53970 and the accuracy is 0.63852\n",
      "VALIDATION: In the epoch   100 the loss is 0.54488 and the accuracy is 0.64108\n",
      "TRAINING: In the epoch   110 the loss is 0.50743 and the accuracy is 0.64394\n",
      "VALIDATION: In the epoch   110 the loss is 0.51203 and the accuracy is 0.64994\n",
      "TRAINING: In the epoch   120 the loss is 0.48047 and the accuracy is 0.65251\n",
      "VALIDATION: In the epoch   120 the loss is 0.48420 and the accuracy is 0.65500\n",
      "TRAINING: In the epoch   130 the loss is 0.45741 and the accuracy is 0.65982\n",
      "VALIDATION: In the epoch   130 the loss is 0.46011 and the accuracy is 0.66132\n",
      "TRAINING: In the epoch   140 the loss is 0.43726 and the accuracy is 0.66657\n",
      "VALIDATION: In the epoch   140 the loss is 0.43892 and the accuracy is 0.66639\n",
      "TRAINING: In the epoch   150 the loss is 0.41945 and the accuracy is 0.67283\n",
      "VALIDATION: In the epoch   150 the loss is 0.42010 and the accuracy is 0.67398\n",
      "TRAINING: In the epoch   160 the loss is 0.40356 and the accuracy is 0.67817\n",
      "VALIDATION: In the epoch   160 the loss is 0.40332 and the accuracy is 0.67862\n",
      "TRAINING: In the epoch   170 the loss is 0.38934 and the accuracy is 0.68351\n",
      "VALIDATION: In the epoch   170 the loss is 0.38831 and the accuracy is 0.68157\n",
      "TRAINING: In the epoch   180 the loss is 0.37657 and the accuracy is 0.68787\n",
      "VALIDATION: In the epoch   180 the loss is 0.37488 and the accuracy is 0.68790\n",
      "TRAINING: In the epoch   190 the loss is 0.36505 and the accuracy is 0.69301\n",
      "VALIDATION: In the epoch   190 the loss is 0.36284 and the accuracy is 0.69591\n",
      "TRAINING: In the epoch   200 the loss is 0.35463 and the accuracy is 0.69821\n",
      "VALIDATION: In the epoch   200 the loss is 0.35203 and the accuracy is 0.70055\n",
      "TRAINING:In the epoch   200 the precission is 0.59244 and the recall is 0.62763\n",
      "VALIDATION:In the epoch   200 the precission is 0.59773 and the recall is 0.63269\n",
      "TRAINING: In the epoch   210 the loss is 0.34517 and the accuracy is 0.70095\n",
      "VALIDATION: In the epoch   210 the loss is 0.34230 and the accuracy is 0.70266\n",
      "TRAINING: In the epoch   220 the loss is 0.33656 and the accuracy is 0.70587\n",
      "VALIDATION: In the epoch   220 the loss is 0.33350 and the accuracy is 0.70645\n",
      "TRAINING: In the epoch   230 the loss is 0.32869 and the accuracy is 0.70981\n",
      "VALIDATION: In the epoch   230 the loss is 0.32554 and the accuracy is 0.70983\n",
      "TRAINING: In the epoch   240 the loss is 0.32148 and the accuracy is 0.71381\n",
      "VALIDATION: In the epoch   240 the loss is 0.31830 and the accuracy is 0.71489\n",
      "TRAINING: In the epoch   250 the loss is 0.31485 and the accuracy is 0.71592\n",
      "VALIDATION: In the epoch   250 the loss is 0.31170 and the accuracy is 0.71489\n",
      "TRAINING: In the epoch   260 the loss is 0.30874 and the accuracy is 0.72021\n",
      "VALIDATION: In the epoch   260 the loss is 0.30565 and the accuracy is 0.71868\n",
      "TRAINING: In the epoch   270 the loss is 0.30309 and the accuracy is 0.72288\n",
      "VALIDATION: In the epoch   270 the loss is 0.30011 and the accuracy is 0.72121\n",
      "TRAINING: In the epoch   280 the loss is 0.29785 and the accuracy is 0.72534\n",
      "VALIDATION: In the epoch   280 the loss is 0.29500 and the accuracy is 0.72459\n",
      "TRAINING: In the epoch   290 the loss is 0.29298 and the accuracy is 0.72858\n",
      "VALIDATION: In the epoch   290 the loss is 0.29028 and the accuracy is 0.72965\n",
      "TRAINING: In the epoch   300 the loss is 0.28845 and the accuracy is 0.73195\n",
      "VALIDATION: In the epoch   300 the loss is 0.28590 and the accuracy is 0.73176\n",
      "TRAINING: In the epoch   310 the loss is 0.28423 and the accuracy is 0.73511\n",
      "VALIDATION: In the epoch   310 the loss is 0.28184 and the accuracy is 0.73176\n",
      "TRAINING: In the epoch   320 the loss is 0.28028 and the accuracy is 0.73750\n",
      "VALIDATION: In the epoch   320 the loss is 0.27806 and the accuracy is 0.73387\n",
      "TRAINING: In the epoch   330 the loss is 0.27658 and the accuracy is 0.74172\n",
      "VALIDATION: In the epoch   330 the loss is 0.27453 and the accuracy is 0.73640\n",
      "TRAINING: In the epoch   340 the loss is 0.27311 and the accuracy is 0.74552\n",
      "VALIDATION: In the epoch   340 the loss is 0.27123 and the accuracy is 0.74019\n",
      "TRAINING: In the epoch   350 the loss is 0.26985 and the accuracy is 0.74868\n",
      "VALIDATION: In the epoch   350 the loss is 0.26814 and the accuracy is 0.74357\n",
      "TRAINING: In the epoch   360 the loss is 0.26679 and the accuracy is 0.75107\n",
      "VALIDATION: In the epoch   360 the loss is 0.26523 and the accuracy is 0.74526\n",
      "TRAINING: In the epoch   370 the loss is 0.26391 and the accuracy is 0.75283\n",
      "VALIDATION: In the epoch   370 the loss is 0.26250 and the accuracy is 0.74821\n",
      "TRAINING: In the epoch   380 the loss is 0.26119 and the accuracy is 0.75402\n",
      "VALIDATION: In the epoch   380 the loss is 0.25992 and the accuracy is 0.74989\n",
      "TRAINING: In the epoch   390 the loss is 0.25863 and the accuracy is 0.75543\n",
      "VALIDATION: In the epoch   390 the loss is 0.25748 and the accuracy is 0.74947\n",
      "TRAINING: In the epoch   400 the loss is 0.25621 and the accuracy is 0.75698\n",
      "VALIDATION: In the epoch   400 the loss is 0.25517 and the accuracy is 0.75032\n",
      "TRAINING:In the epoch   400 the precission is 0.71333 and the recall is 0.74842\n",
      "VALIDATION:In the epoch   400 the precission is 0.66182 and the recall is 0.70115\n",
      "TRAINING: In the epoch   410 the loss is 0.25391 and the accuracy is 0.75761\n",
      "VALIDATION: In the epoch   410 the loss is 0.25298 and the accuracy is 0.74989\n",
      "TRAINING: In the epoch   420 the loss is 0.25174 and the accuracy is 0.75944\n",
      "VALIDATION: In the epoch   420 the loss is 0.25091 and the accuracy is 0.75032\n",
      "TRAINING: In the epoch   430 the loss is 0.24968 and the accuracy is 0.76007\n",
      "VALIDATION: In the epoch   430 the loss is 0.24893 and the accuracy is 0.75243\n",
      "TRAINING: In the epoch   440 the loss is 0.24773 and the accuracy is 0.76162\n",
      "VALIDATION: In the epoch   440 the loss is 0.24705 and the accuracy is 0.75200\n",
      "TRAINING: In the epoch   450 the loss is 0.24587 and the accuracy is 0.76288\n",
      "VALIDATION: In the epoch   450 the loss is 0.24526 and the accuracy is 0.75285\n",
      "TRAINING: In the epoch   460 the loss is 0.24410 and the accuracy is 0.76380\n",
      "VALIDATION: In the epoch   460 the loss is 0.24355 and the accuracy is 0.75327\n",
      "TRAINING: In the epoch   470 the loss is 0.24241 and the accuracy is 0.76478\n",
      "VALIDATION: In the epoch   470 the loss is 0.24192 and the accuracy is 0.75327\n",
      "TRAINING: In the epoch   480 the loss is 0.24080 and the accuracy is 0.76534\n",
      "VALIDATION: In the epoch   480 the loss is 0.24036 and the accuracy is 0.75411\n",
      "TRAINING: In the epoch   490 the loss is 0.23926 and the accuracy is 0.76598\n",
      "VALIDATION: In the epoch   490 the loss is 0.23886 and the accuracy is 0.75369\n",
      "TRAINING: In the epoch   500 the loss is 0.23779 and the accuracy is 0.76647\n",
      "VALIDATION: In the epoch   500 the loss is 0.23743 and the accuracy is 0.75369\n",
      "TRAINING: In the epoch   510 the loss is 0.23639 and the accuracy is 0.76661\n",
      "VALIDATION: In the epoch   510 the loss is 0.23605 and the accuracy is 0.75411\n",
      "TRAINING: In the epoch   520 the loss is 0.23504 and the accuracy is 0.76738\n",
      "VALIDATION: In the epoch   520 the loss is 0.23472 and the accuracy is 0.75496\n",
      "TRAINING: In the epoch   530 the loss is 0.23374 and the accuracy is 0.76865\n",
      "VALIDATION: In the epoch   530 the loss is 0.23345 and the accuracy is 0.75411\n",
      "TRAINING: In the epoch   540 the loss is 0.23249 and the accuracy is 0.76991\n",
      "VALIDATION: In the epoch   540 the loss is 0.23222 and the accuracy is 0.75243\n",
      "TRAINING: In the epoch   550 the loss is 0.23130 and the accuracy is 0.77026\n",
      "VALIDATION: In the epoch   550 the loss is 0.23104 and the accuracy is 0.75243\n",
      "TRAINING: In the epoch   560 the loss is 0.23014 and the accuracy is 0.77069\n",
      "VALIDATION: In the epoch   560 the loss is 0.22989 and the accuracy is 0.75243\n",
      "TRAINING: In the epoch   570 the loss is 0.22903 and the accuracy is 0.77167\n",
      "VALIDATION: In the epoch   570 the loss is 0.22879 and the accuracy is 0.75453\n",
      "TRAINING: In the epoch   580 the loss is 0.22796 and the accuracy is 0.77272\n",
      "VALIDATION: In the epoch   580 the loss is 0.22772 and the accuracy is 0.75580\n",
      "TRAINING: In the epoch   590 the loss is 0.22692 and the accuracy is 0.77322\n",
      "VALIDATION: In the epoch   590 the loss is 0.22668 and the accuracy is 0.75580\n",
      "TRAINING: In the epoch   600 the loss is 0.22592 and the accuracy is 0.77357\n",
      "VALIDATION: In the epoch   600 the loss is 0.22568 and the accuracy is 0.75622\n",
      "TRAINING:In the epoch   600 the precission is 0.72646 and the recall is 0.75342\n",
      "VALIDATION:In the epoch   600 the precission is 0.65935 and the recall is 0.71397\n",
      "TRAINING: In the epoch   610 the loss is 0.22496 and the accuracy is 0.77427\n",
      "VALIDATION: In the epoch   610 the loss is 0.22471 and the accuracy is 0.75664\n",
      "TRAINING: In the epoch   620 the loss is 0.22402 and the accuracy is 0.77511\n",
      "VALIDATION: In the epoch   620 the loss is 0.22377 and the accuracy is 0.75791\n",
      "TRAINING: In the epoch   630 the loss is 0.22312 and the accuracy is 0.77631\n",
      "VALIDATION: In the epoch   630 the loss is 0.22286 and the accuracy is 0.75960\n",
      "TRAINING: In the epoch   640 the loss is 0.22224 and the accuracy is 0.77701\n",
      "VALIDATION: In the epoch   640 the loss is 0.22197 and the accuracy is 0.75917\n",
      "TRAINING: In the epoch   650 the loss is 0.22139 and the accuracy is 0.77729\n",
      "VALIDATION: In the epoch   650 the loss is 0.22111 and the accuracy is 0.76128\n",
      "TRAINING: In the epoch   660 the loss is 0.22056 and the accuracy is 0.77814\n",
      "VALIDATION: In the epoch   660 the loss is 0.22027 and the accuracy is 0.76297\n",
      "TRAINING: In the epoch   670 the loss is 0.21976 and the accuracy is 0.77863\n",
      "VALIDATION: In the epoch   670 the loss is 0.21946 and the accuracy is 0.76339\n",
      "TRAINING: In the epoch   680 the loss is 0.21898 and the accuracy is 0.77933\n",
      "VALIDATION: In the epoch   680 the loss is 0.21867 and the accuracy is 0.76423\n",
      "TRAINING: In the epoch   690 the loss is 0.21822 and the accuracy is 0.77947\n",
      "VALIDATION: In the epoch   690 the loss is 0.21790 and the accuracy is 0.76423\n",
      "TRAINING: In the epoch   700 the loss is 0.21748 and the accuracy is 0.78025\n",
      "VALIDATION: In the epoch   700 the loss is 0.21715 and the accuracy is 0.76466\n",
      "TRAINING: In the epoch   710 the loss is 0.21677 and the accuracy is 0.78074\n",
      "VALIDATION: In the epoch   710 the loss is 0.21643 and the accuracy is 0.76550\n",
      "TRAINING: In the epoch   720 the loss is 0.21607 and the accuracy is 0.78109\n",
      "VALIDATION: In the epoch   720 the loss is 0.21572 and the accuracy is 0.76761\n",
      "TRAINING: In the epoch   730 the loss is 0.21539 and the accuracy is 0.78137\n",
      "VALIDATION: In the epoch   730 the loss is 0.21503 and the accuracy is 0.76761\n",
      "TRAINING: In the epoch   740 the loss is 0.21473 and the accuracy is 0.78186\n",
      "VALIDATION: In the epoch   740 the loss is 0.21437 and the accuracy is 0.76803\n",
      "TRAINING: In the epoch   750 the loss is 0.21408 and the accuracy is 0.78236\n",
      "VALIDATION: In the epoch   750 the loss is 0.21372 and the accuracy is 0.76761\n",
      "TRAINING: In the epoch   760 the loss is 0.21345 and the accuracy is 0.78285\n",
      "VALIDATION: In the epoch   760 the loss is 0.21309 and the accuracy is 0.76845\n",
      "TRAINING: In the epoch   770 the loss is 0.21284 and the accuracy is 0.78341\n",
      "VALIDATION: In the epoch   770 the loss is 0.21247 and the accuracy is 0.76972\n",
      "TRAINING: In the epoch   780 the loss is 0.21224 and the accuracy is 0.78390\n",
      "VALIDATION: In the epoch   780 the loss is 0.21187 and the accuracy is 0.76972\n",
      "TRAINING: In the epoch   790 the loss is 0.21166 and the accuracy is 0.78404\n",
      "VALIDATION: In the epoch   790 the loss is 0.21129 and the accuracy is 0.76930\n",
      "TRAINING: In the epoch   800 the loss is 0.21109 and the accuracy is 0.78475\n",
      "VALIDATION: In the epoch   800 the loss is 0.21073 and the accuracy is 0.76972\n",
      "TRAINING:In the epoch   800 the precission is 0.74384 and the recall is 0.78974\n",
      "VALIDATION:In the epoch   800 the precission is 0.67750 and the recall is 0.74962\n",
      "TRAINING: In the epoch   810 the loss is 0.21053 and the accuracy is 0.78489\n",
      "VALIDATION: In the epoch   810 the loss is 0.21018 and the accuracy is 0.77014\n",
      "TRAINING: In the epoch   820 the loss is 0.20999 and the accuracy is 0.78531\n",
      "VALIDATION: In the epoch   820 the loss is 0.20965 and the accuracy is 0.77056\n",
      "TRAINING: In the epoch   830 the loss is 0.20946 and the accuracy is 0.78552\n",
      "VALIDATION: In the epoch   830 the loss is 0.20913 and the accuracy is 0.77225\n",
      "TRAINING: In the epoch   840 the loss is 0.20895 and the accuracy is 0.78566\n",
      "VALIDATION: In the epoch   840 the loss is 0.20862 and the accuracy is 0.77309\n",
      "TRAINING: In the epoch   850 the loss is 0.20844 and the accuracy is 0.78587\n",
      "VALIDATION: In the epoch   850 the loss is 0.20813 and the accuracy is 0.77520\n",
      "TRAINING: In the epoch   860 the loss is 0.20795 and the accuracy is 0.78657\n",
      "VALIDATION: In the epoch   860 the loss is 0.20765 and the accuracy is 0.77604\n",
      "TRAINING: In the epoch   870 the loss is 0.20747 and the accuracy is 0.78685\n",
      "VALIDATION: In the epoch   870 the loss is 0.20719 and the accuracy is 0.77604\n",
      "TRAINING: In the epoch   880 the loss is 0.20700 and the accuracy is 0.78707\n",
      "VALIDATION: In the epoch   880 the loss is 0.20673 and the accuracy is 0.77647\n",
      "TRAINING: In the epoch   890 the loss is 0.20653 and the accuracy is 0.78770\n",
      "VALIDATION: In the epoch   890 the loss is 0.20629 and the accuracy is 0.77689\n",
      "TRAINING: In the epoch   900 the loss is 0.20608 and the accuracy is 0.78749\n",
      "VALIDATION: In the epoch   900 the loss is 0.20586 and the accuracy is 0.77773\n",
      "TRAINING: In the epoch   910 the loss is 0.20564 and the accuracy is 0.78770\n",
      "VALIDATION: In the epoch   910 the loss is 0.20544 and the accuracy is 0.77942\n",
      "TRAINING: In the epoch   920 the loss is 0.20521 and the accuracy is 0.78791\n",
      "VALIDATION: In the epoch   920 the loss is 0.20503 and the accuracy is 0.77942\n",
      "TRAINING: In the epoch   930 the loss is 0.20478 and the accuracy is 0.78798\n",
      "VALIDATION: In the epoch   930 the loss is 0.20463 and the accuracy is 0.77942\n",
      "TRAINING: In the epoch   940 the loss is 0.20437 and the accuracy is 0.78833\n",
      "VALIDATION: In the epoch   940 the loss is 0.20425 and the accuracy is 0.77942\n",
      "TRAINING: In the epoch   950 the loss is 0.20396 and the accuracy is 0.78826\n",
      "VALIDATION: In the epoch   950 the loss is 0.20387 and the accuracy is 0.78026\n",
      "TRAINING: In the epoch   960 the loss is 0.20356 and the accuracy is 0.78868\n",
      "VALIDATION: In the epoch   960 the loss is 0.20349 and the accuracy is 0.78111\n",
      "TRAINING: In the epoch   970 the loss is 0.20317 and the accuracy is 0.78938\n",
      "VALIDATION: In the epoch   970 the loss is 0.20313 and the accuracy is 0.78237\n",
      "TRAINING: In the epoch   980 the loss is 0.20278 and the accuracy is 0.78981\n",
      "VALIDATION: In the epoch   980 the loss is 0.20277 and the accuracy is 0.78195\n",
      "TRAINING: In the epoch   990 the loss is 0.20241 and the accuracy is 0.78995\n",
      "VALIDATION: In the epoch   990 the loss is 0.20243 and the accuracy is 0.78237\n",
      "TRAINING: In the epoch  1000 the loss is 0.20203 and the accuracy is 0.78988\n",
      "VALIDATION: In the epoch  1000 the loss is 0.20209 and the accuracy is 0.78321\n",
      "TRAINING:In the epoch  1000 the precission is 0.76547 and the recall is 0.81289\n",
      "VALIDATION:In the epoch  1000 the precission is 0.69792 and the recall is 0.77244\n",
      "TRAINING: In the epoch  1010 the loss is 0.20167 and the accuracy is 0.79051\n",
      "VALIDATION: In the epoch  1010 the loss is 0.20175 and the accuracy is 0.78321\n",
      "TRAINING: In the epoch  1020 the loss is 0.20131 and the accuracy is 0.79023\n",
      "VALIDATION: In the epoch  1020 the loss is 0.20142 and the accuracy is 0.78279\n",
      "TRAINING: In the epoch  1030 the loss is 0.20096 and the accuracy is 0.79037\n",
      "VALIDATION: In the epoch  1030 the loss is 0.20110 and the accuracy is 0.78237\n",
      "TRAINING: In the epoch  1040 the loss is 0.20061 and the accuracy is 0.79044\n",
      "VALIDATION: In the epoch  1040 the loss is 0.20078 and the accuracy is 0.78321\n",
      "TRAINING: In the epoch  1050 the loss is 0.20027 and the accuracy is 0.79030\n",
      "VALIDATION: In the epoch  1050 the loss is 0.20047 and the accuracy is 0.78321\n",
      "TRAINING: In the epoch  1060 the loss is 0.19994 and the accuracy is 0.79037\n",
      "VALIDATION: In the epoch  1060 the loss is 0.20017 and the accuracy is 0.78279\n",
      "TRAINING: In the epoch  1070 the loss is 0.19961 and the accuracy is 0.79079\n",
      "VALIDATION: In the epoch  1070 the loss is 0.19987 and the accuracy is 0.78364\n",
      "TRAINING: In the epoch  1080 the loss is 0.19928 and the accuracy is 0.79114\n",
      "VALIDATION: In the epoch  1080 the loss is 0.19957 and the accuracy is 0.78237\n",
      "TRAINING: In the epoch  1090 the loss is 0.19896 and the accuracy is 0.79114\n",
      "VALIDATION: In the epoch  1090 the loss is 0.19928 and the accuracy is 0.78237\n",
      "TRAINING: In the epoch  1100 the loss is 0.19865 and the accuracy is 0.79142\n",
      "VALIDATION: In the epoch  1100 the loss is 0.19899 and the accuracy is 0.78237\n",
      "TRAINING: In the epoch  1110 the loss is 0.19834 and the accuracy is 0.79156\n",
      "VALIDATION: In the epoch  1110 the loss is 0.19871 and the accuracy is 0.78364\n",
      "TRAINING: In the epoch  1120 the loss is 0.19804 and the accuracy is 0.79192\n",
      "VALIDATION: In the epoch  1120 the loss is 0.19843 and the accuracy is 0.78448\n",
      "TRAINING: In the epoch  1130 the loss is 0.19774 and the accuracy is 0.79227\n",
      "VALIDATION: In the epoch  1130 the loss is 0.19815 and the accuracy is 0.78490\n",
      "TRAINING: In the epoch  1140 the loss is 0.19745 and the accuracy is 0.79227\n",
      "VALIDATION: In the epoch  1140 the loss is 0.19788 and the accuracy is 0.78574\n",
      "TRAINING: In the epoch  1150 the loss is 0.19716 and the accuracy is 0.79227\n",
      "VALIDATION: In the epoch  1150 the loss is 0.19762 and the accuracy is 0.78574\n",
      "TRAINING: In the epoch  1160 the loss is 0.19687 and the accuracy is 0.79248\n",
      "VALIDATION: In the epoch  1160 the loss is 0.19735 and the accuracy is 0.78574\n",
      "TRAINING: In the epoch  1170 the loss is 0.19659 and the accuracy is 0.79283\n",
      "VALIDATION: In the epoch  1170 the loss is 0.19709 and the accuracy is 0.78574\n",
      "TRAINING: In the epoch  1180 the loss is 0.19632 and the accuracy is 0.79304\n",
      "VALIDATION: In the epoch  1180 the loss is 0.19684 and the accuracy is 0.78617\n",
      "TRAINING: In the epoch  1190 the loss is 0.19605 and the accuracy is 0.79381\n",
      "VALIDATION: In the epoch  1190 the loss is 0.19658 and the accuracy is 0.78659\n",
      "TRAINING: In the epoch  1200 the loss is 0.19579 and the accuracy is 0.79381\n",
      "VALIDATION: In the epoch  1200 the loss is 0.19633 and the accuracy is 0.78743\n",
      "TRAINING:In the epoch  1200 the precission is 0.76547 and the recall is 0.81289\n",
      "VALIDATION:In the epoch  1200 the precission is 0.69792 and the recall is 0.77244\n",
      "TRAINING: In the epoch  1210 the loss is 0.19552 and the accuracy is 0.79374\n",
      "VALIDATION: In the epoch  1210 the loss is 0.19609 and the accuracy is 0.78743\n",
      "TRAINING: In the epoch  1220 the loss is 0.19527 and the accuracy is 0.79353\n",
      "VALIDATION: In the epoch  1220 the loss is 0.19584 and the accuracy is 0.78743\n",
      "TRAINING: In the epoch  1230 the loss is 0.19502 and the accuracy is 0.79325\n",
      "VALIDATION: In the epoch  1230 the loss is 0.19561 and the accuracy is 0.78659\n",
      "TRAINING: In the epoch  1240 the loss is 0.19477 and the accuracy is 0.79339\n",
      "VALIDATION: In the epoch  1240 the loss is 0.19537 and the accuracy is 0.78743\n",
      "TRAINING: In the epoch  1250 the loss is 0.19453 and the accuracy is 0.79332\n",
      "VALIDATION: In the epoch  1250 the loss is 0.19514 and the accuracy is 0.78870\n",
      "TRAINING: In the epoch  1260 the loss is 0.19429 and the accuracy is 0.79325\n",
      "VALIDATION: In the epoch  1260 the loss is 0.19491 and the accuracy is 0.78912\n",
      "TRAINING: In the epoch  1270 the loss is 0.19405 and the accuracy is 0.79339\n",
      "VALIDATION: In the epoch  1270 the loss is 0.19469 and the accuracy is 0.78912\n",
      "TRAINING: In the epoch  1280 the loss is 0.19382 and the accuracy is 0.79339\n",
      "VALIDATION: In the epoch  1280 the loss is 0.19446 and the accuracy is 0.78954\n",
      "TRAINING: In the epoch  1290 the loss is 0.19360 and the accuracy is 0.79367\n",
      "VALIDATION: In the epoch  1290 the loss is 0.19425 and the accuracy is 0.79038\n",
      "TRAINING: In the epoch  1300 the loss is 0.19338 and the accuracy is 0.79388\n",
      "VALIDATION: In the epoch  1300 the loss is 0.19403 and the accuracy is 0.79038\n",
      "TRAINING: In the epoch  1310 the loss is 0.19316 and the accuracy is 0.79395\n",
      "VALIDATION: In the epoch  1310 the loss is 0.19382 and the accuracy is 0.79081\n",
      "TRAINING: In the epoch  1320 the loss is 0.19294 and the accuracy is 0.79417\n",
      "VALIDATION: In the epoch  1320 the loss is 0.19361 and the accuracy is 0.79081\n",
      "TRAINING: In the epoch  1330 the loss is 0.19273 and the accuracy is 0.79417\n",
      "VALIDATION: In the epoch  1330 the loss is 0.19341 and the accuracy is 0.79038\n",
      "TRAINING: In the epoch  1340 the loss is 0.19253 and the accuracy is 0.79431\n",
      "VALIDATION: In the epoch  1340 the loss is 0.19321 and the accuracy is 0.79081\n",
      "TRAINING: In the epoch  1350 the loss is 0.19232 and the accuracy is 0.79459\n",
      "VALIDATION: In the epoch  1350 the loss is 0.19301 and the accuracy is 0.79081\n",
      "TRAINING: In the epoch  1360 the loss is 0.19212 and the accuracy is 0.79445\n",
      "VALIDATION: In the epoch  1360 the loss is 0.19281 and the accuracy is 0.79123\n",
      "TRAINING: In the epoch  1370 the loss is 0.19193 and the accuracy is 0.79459\n",
      "VALIDATION: In the epoch  1370 the loss is 0.19262 and the accuracy is 0.79165\n",
      "TRAINING: In the epoch  1380 the loss is 0.19174 and the accuracy is 0.79431\n",
      "VALIDATION: In the epoch  1380 the loss is 0.19243 and the accuracy is 0.79165\n",
      "TRAINING: In the epoch  1390 the loss is 0.19155 and the accuracy is 0.79445\n",
      "VALIDATION: In the epoch  1390 the loss is 0.19225 and the accuracy is 0.79207\n",
      "TRAINING: In the epoch  1400 the loss is 0.19136 and the accuracy is 0.79473\n",
      "VALIDATION: In the epoch  1400 the loss is 0.19206 and the accuracy is 0.79207\n",
      "TRAINING:In the epoch  1400 the precission is 0.78709 and the recall is 0.83605\n",
      "VALIDATION:In the epoch  1400 the precission is 0.70979 and the recall is 0.77744\n",
      "TRAINING: In the epoch  1410 the loss is 0.19118 and the accuracy is 0.79501\n",
      "VALIDATION: In the epoch  1410 the loss is 0.19188 and the accuracy is 0.79291\n",
      "TRAINING: In the epoch  1420 the loss is 0.19100 and the accuracy is 0.79508\n",
      "VALIDATION: In the epoch  1420 the loss is 0.19170 and the accuracy is 0.79334\n",
      "TRAINING: In the epoch  1430 the loss is 0.19082 and the accuracy is 0.79529\n",
      "VALIDATION: In the epoch  1430 the loss is 0.19153 and the accuracy is 0.79376\n",
      "TRAINING: In the epoch  1440 the loss is 0.19065 and the accuracy is 0.79536\n",
      "VALIDATION: In the epoch  1440 the loss is 0.19136 and the accuracy is 0.79376\n",
      "TRAINING: In the epoch  1450 the loss is 0.19047 and the accuracy is 0.79585\n",
      "VALIDATION: In the epoch  1450 the loss is 0.19119 and the accuracy is 0.79418\n",
      "TRAINING: In the epoch  1460 the loss is 0.19031 and the accuracy is 0.79606\n",
      "VALIDATION: In the epoch  1460 the loss is 0.19102 and the accuracy is 0.79460\n",
      "TRAINING: In the epoch  1470 the loss is 0.19014 and the accuracy is 0.79627\n",
      "VALIDATION: In the epoch  1470 the loss is 0.19085 and the accuracy is 0.79460\n",
      "TRAINING: In the epoch  1480 the loss is 0.18998 and the accuracy is 0.79627\n",
      "VALIDATION: In the epoch  1480 the loss is 0.19069 and the accuracy is 0.79460\n",
      "TRAINING: In the epoch  1490 the loss is 0.18981 and the accuracy is 0.79656\n",
      "VALIDATION: In the epoch  1490 the loss is 0.19053 and the accuracy is 0.79502\n",
      "TRAINING: In the epoch  1500 the loss is 0.18965 and the accuracy is 0.79649\n",
      "VALIDATION: In the epoch  1500 the loss is 0.19037 and the accuracy is 0.79544\n",
      "TRAINING: In the epoch  1510 the loss is 0.18950 and the accuracy is 0.79677\n",
      "VALIDATION: In the epoch  1510 the loss is 0.19021 and the accuracy is 0.79544\n",
      "TRAINING: In the epoch  1520 the loss is 0.18934 and the accuracy is 0.79698\n",
      "VALIDATION: In the epoch  1520 the loss is 0.19006 and the accuracy is 0.79544\n",
      "TRAINING: In the epoch  1530 the loss is 0.18919 and the accuracy is 0.79719\n",
      "VALIDATION: In the epoch  1530 the loss is 0.18991 and the accuracy is 0.79502\n",
      "TRAINING: In the epoch  1540 the loss is 0.18904 and the accuracy is 0.79705\n",
      "VALIDATION: In the epoch  1540 the loss is 0.18976 and the accuracy is 0.79544\n",
      "TRAINING: In the epoch  1550 the loss is 0.18889 and the accuracy is 0.79719\n",
      "VALIDATION: In the epoch  1550 the loss is 0.18961 and the accuracy is 0.79629\n",
      "TRAINING: In the epoch  1560 the loss is 0.18875 and the accuracy is 0.79747\n",
      "VALIDATION: In the epoch  1560 the loss is 0.18946 and the accuracy is 0.79544\n",
      "TRAINING: In the epoch  1570 the loss is 0.18860 and the accuracy is 0.79775\n",
      "VALIDATION: In the epoch  1570 the loss is 0.18932 and the accuracy is 0.79587\n",
      "TRAINING: In the epoch  1580 the loss is 0.18846 and the accuracy is 0.79789\n",
      "VALIDATION: In the epoch  1580 the loss is 0.18917 and the accuracy is 0.79629\n",
      "TRAINING: In the epoch  1590 the loss is 0.18832 and the accuracy is 0.79831\n",
      "VALIDATION: In the epoch  1590 the loss is 0.18903 and the accuracy is 0.79629\n",
      "TRAINING: In the epoch  1600 the loss is 0.18818 and the accuracy is 0.79838\n",
      "VALIDATION: In the epoch  1600 the loss is 0.18889 and the accuracy is 0.79671\n",
      "TRAINING:In the epoch  1600 the precission is 0.81043 and the recall is 0.89053\n",
      "VALIDATION:In the epoch  1600 the precission is 0.71833 and the recall is 0.79526\n",
      "TRAINING: In the epoch  1610 the loss is 0.18804 and the accuracy is 0.79888\n",
      "VALIDATION: In the epoch  1610 the loss is 0.18875 and the accuracy is 0.79629\n",
      "TRAINING: In the epoch  1620 the loss is 0.18791 and the accuracy is 0.79909\n",
      "VALIDATION: In the epoch  1620 the loss is 0.18862 and the accuracy is 0.79629\n",
      "TRAINING: In the epoch  1630 the loss is 0.18777 and the accuracy is 0.79895\n",
      "VALIDATION: In the epoch  1630 the loss is 0.18848 and the accuracy is 0.79629\n",
      "TRAINING: In the epoch  1640 the loss is 0.18764 and the accuracy is 0.79916\n",
      "VALIDATION: In the epoch  1640 the loss is 0.18835 and the accuracy is 0.79629\n",
      "TRAINING: In the epoch  1650 the loss is 0.18751 and the accuracy is 0.79923\n",
      "VALIDATION: In the epoch  1650 the loss is 0.18822 and the accuracy is 0.79671\n",
      "TRAINING: In the epoch  1660 the loss is 0.18738 and the accuracy is 0.79916\n",
      "VALIDATION: In the epoch  1660 the loss is 0.18809 and the accuracy is 0.79713\n",
      "TRAINING: In the epoch  1670 the loss is 0.18726 and the accuracy is 0.79923\n",
      "VALIDATION: In the epoch  1670 the loss is 0.18796 and the accuracy is 0.79713\n",
      "TRAINING: In the epoch  1680 the loss is 0.18713 and the accuracy is 0.79909\n",
      "VALIDATION: In the epoch  1680 the loss is 0.18783 and the accuracy is 0.79755\n",
      "TRAINING: In the epoch  1690 the loss is 0.18701 and the accuracy is 0.79930\n",
      "VALIDATION: In the epoch  1690 the loss is 0.18771 and the accuracy is 0.79755\n",
      "TRAINING: In the epoch  1700 the loss is 0.18688 and the accuracy is 0.79909\n",
      "VALIDATION: In the epoch  1700 the loss is 0.18758 and the accuracy is 0.79755\n",
      "TRAINING: In the epoch  1710 the loss is 0.18676 and the accuracy is 0.79909\n",
      "VALIDATION: In the epoch  1710 the loss is 0.18746 and the accuracy is 0.79713\n",
      "TRAINING: In the epoch  1720 the loss is 0.18664 and the accuracy is 0.79937\n",
      "VALIDATION: In the epoch  1720 the loss is 0.18734 and the accuracy is 0.79798\n",
      "TRAINING: In the epoch  1730 the loss is 0.18652 and the accuracy is 0.79958\n",
      "VALIDATION: In the epoch  1730 the loss is 0.18722 and the accuracy is 0.79755\n",
      "TRAINING: In the epoch  1740 the loss is 0.18641 and the accuracy is 0.79958\n",
      "VALIDATION: In the epoch  1740 the loss is 0.18710 and the accuracy is 0.79798\n",
      "TRAINING: In the epoch  1750 the loss is 0.18629 and the accuracy is 0.80000\n",
      "VALIDATION: In the epoch  1750 the loss is 0.18698 and the accuracy is 0.79798\n",
      "TRAINING: In the epoch  1760 the loss is 0.18617 and the accuracy is 0.80021\n",
      "VALIDATION: In the epoch  1760 the loss is 0.18687 and the accuracy is 0.79840\n",
      "TRAINING: In the epoch  1770 the loss is 0.18606 and the accuracy is 0.80049\n",
      "VALIDATION: In the epoch  1770 the loss is 0.18675 and the accuracy is 0.79840\n",
      "TRAINING: In the epoch  1780 the loss is 0.18595 and the accuracy is 0.80063\n",
      "VALIDATION: In the epoch  1780 the loss is 0.18664 and the accuracy is 0.79882\n",
      "TRAINING: In the epoch  1790 the loss is 0.18584 and the accuracy is 0.80105\n",
      "VALIDATION: In the epoch  1790 the loss is 0.18652 and the accuracy is 0.79882\n",
      "TRAINING: In the epoch  1800 the loss is 0.18573 and the accuracy is 0.80112\n",
      "VALIDATION: In the epoch  1800 the loss is 0.18641 and the accuracy is 0.79882\n",
      "TRAINING:In the epoch  1800 the precission is 0.82542 and the recall is 0.92684\n",
      "VALIDATION:In the epoch  1800 the precission is 0.74283 and the recall is 0.80526\n",
      "TRAINING: In the epoch  1810 the loss is 0.18562 and the accuracy is 0.80134\n",
      "VALIDATION: In the epoch  1810 the loss is 0.18630 and the accuracy is 0.79882\n",
      "TRAINING: In the epoch  1820 the loss is 0.18551 and the accuracy is 0.80141\n",
      "VALIDATION: In the epoch  1820 the loss is 0.18619 and the accuracy is 0.79924\n",
      "TRAINING: In the epoch  1830 the loss is 0.18540 and the accuracy is 0.80141\n",
      "VALIDATION: In the epoch  1830 the loss is 0.18608 and the accuracy is 0.79924\n",
      "TRAINING: In the epoch  1840 the loss is 0.18530 and the accuracy is 0.80162\n",
      "VALIDATION: In the epoch  1840 the loss is 0.18597 and the accuracy is 0.79924\n",
      "TRAINING: In the epoch  1850 the loss is 0.18519 and the accuracy is 0.80190\n",
      "VALIDATION: In the epoch  1850 the loss is 0.18587 and the accuracy is 0.80008\n",
      "TRAINING: In the epoch  1860 the loss is 0.18509 and the accuracy is 0.80183\n",
      "VALIDATION: In the epoch  1860 the loss is 0.18576 and the accuracy is 0.80051\n",
      "TRAINING: In the epoch  1870 the loss is 0.18499 and the accuracy is 0.80232\n",
      "VALIDATION: In the epoch  1870 the loss is 0.18565 and the accuracy is 0.80051\n",
      "TRAINING: In the epoch  1880 the loss is 0.18489 and the accuracy is 0.80246\n",
      "VALIDATION: In the epoch  1880 the loss is 0.18555 and the accuracy is 0.80051\n",
      "At the final epoch  1885 the precission is 0.82542 and the recall is 0.92684\n"
     ]
    }
   ],
   "source": [
    "x_trainp,y_trainp=jnp.transpose(x_train),jnp.transpose(y_train)\n",
    "layers=[x_trainp.shape[0],30,30,2]\n",
    "lr=0.01\n",
    "labels=jnp.array([0,1])\n",
    "k_clases=2\n",
    "samples=x.shape[0]\n",
    "y_hotp=jnp.transpose(hot.one_hot(y_train,2))\n",
    "stop=0.00001\n",
    "max_steps=10000\n",
    "x_valp,y_valp,y_hot_valp,samples_val=jnp.transpose(x_v),jnp.transpose(y_v),jnp.transpose(hot.one_hot(y_v,2)),x_v.shape[0]\n",
    "run_name='MLP'\n",
    "mlp_mlflow=MultilayerPerceptron(layers,lr,labels,k_clases,samples,x_trainp,y_trainp,y_hotp,stop,max_steps)\n",
    "loss,recall_list,precision_list,loss_list,precision_list_val,recall_list_val,loss_list_val=mlp_mlflow.modelo(mlp_mlflow.weights,max_steps,x_trainp,y_trainp,y_hotp,lr,k_clases,samples,labels,stop,x_valp,y_valp,y_hot_valp,samples_val,run_name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Model\n",
    "================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9740, 79) (9740,)\n",
      "(9740, 79) (9740, 1) 79\n"
     ]
    }
   ],
   "source": [
    "dim=int(x_train.shape[1])\n",
    "y_trainl=y_train\n",
    "y_vl=y_v\n",
    "\n",
    "for i in range(9740):\n",
    "    if y_train[i]==0:\n",
    "        y_trainl[i]==-1\n",
    "for i in range(1624):\n",
    "    if y_v[i]==0:\n",
    "        y_vl[i]=-1\n",
    "\n",
    "x_trainl=jnp.array(x_train)\n",
    "y_trainl=jnp.array(y_trainl)\n",
    "x_vl=jnp.array(x_v)\n",
    "y_vl=jnp.array(y_vl)\n",
    "labels=jnp.array([0,1])\n",
    "k_classes=2\n",
    "n_steps=1000\n",
    "lr=1e-6\n",
    "samples=x_trainl.shape[0]\n",
    "#linear model\n",
    "#importlib.reload(linear)\n",
    "modelo=Linear_Model(dim)\n",
    "theta = modelo.generate_theta() #inicializamos theta con valores aleatorios\n",
    "print(x_trainl.shape, y_trainl.shape)\n",
    "y_trainl=jnp.reshape(y_trainl,(9740,1))\n",
    "print(x_trainl.shape, y_trainl.shape,dim)\n",
    "#theta = modelo.gradient_descent(theta, x_trainl, y_trainl, 10, lr = 0.01)\n",
    "#y_hat=modelo.estimate_grsl(x_trainl,theta)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "este es lr\n",
      "este es el error en el paso i 0 43401.574\n",
      "este es el error en el paso i 1 35594.105\n",
      "este es el error en el paso i 2 32194.805\n",
      "este es el error en el paso i 3 30336.92\n",
      "este es el error en el paso i 4 29054.17\n",
      "este es el error en el paso i 5 28017.623\n",
      "este es el error en el paso i 6 27112.299\n",
      "este es el error en el paso i 7 26295.375\n",
      "este es el error en el paso i 8 25548.832\n",
      "este es el error en el paso i 9 24862.883\n",
      "este es el error en el paso i 10 24230.96\n",
      "este es el error en el paso i 11 23648.262\n",
      "este es el error en el paso i 12 23110.244\n",
      "este es el error en el paso i 13 22613.117\n",
      "este es el error en el paso i 14 22153.184\n",
      "este es el error en el paso i 15 21727.514\n",
      "este es el error en el paso i 16 21333.047\n",
      "este es el error en el paso i 17 20967.111\n",
      "este es el error en el paso i 18 20627.445\n",
      "este es el error en el paso i 19 20311.713\n",
      "este es el error en el paso i 20 20017.963\n",
      "este es el error en el paso i 21 19744.246\n",
      "este es el error en el paso i 22 19489.066\n",
      "este es el error en el paso i 23 19250.883\n",
      "este es el error en el paso i 24 19028.186\n",
      "este es el error en el paso i 25 18819.834\n",
      "este es el error en el paso i 26 18624.52\n",
      "este es el error en el paso i 27 18441.406\n",
      "este es el error en el paso i 28 18269.36\n",
      "este es el error en el paso i 29 18107.559\n",
      "este es el error en el paso i 30 17955.127\n",
      "este es el error en el paso i 31 17811.434\n",
      "este es el error en el paso i 32 17675.707\n",
      "este es el error en el paso i 33 17547.395\n",
      "este es el error en el paso i 34 17425.928\n",
      "este es el error en el paso i 35 17310.67\n",
      "este es el error en el paso i 36 17201.18\n",
      "este es el error en el paso i 37 17097.152\n",
      "este es el error en el paso i 38 16998.137\n",
      "este es el error en el paso i 39 16903.701\n",
      "este es el error en el paso i 40 16813.604\n",
      "este es el error en el paso i 41 16727.377\n",
      "este es el error en el paso i 42 16644.893\n",
      "este es el error en el paso i 43 16565.805\n",
      "este es el error en el paso i 44 16489.94\n",
      "este es el error en el paso i 45 16416.967\n",
      "este es el error en el paso i 46 16346.796\n",
      "este es el error en el paso i 47 16279.115\n",
      "este es el error en el paso i 48 16213.896\n",
      "este es el error en el paso i 49 16150.858\n",
      "este es el error en el paso i 50 16089.991\n",
      "este es el error en el paso i 51 16031.004\n",
      "este es el error en el paso i 52 15973.844\n",
      "este es el error en el paso i 53 15918.404\n",
      "este es el error en el paso i 54 15864.525\n",
      "este es el error en el paso i 55 15812.178\n",
      "este es el error en el paso i 56 15761.26\n",
      "este es el error en el paso i 57 15711.603\n",
      "este es el error en el paso i 58 15663.195\n",
      "este es el error en el paso i 59 15615.968\n",
      "este es el error en el paso i 60 15569.87\n",
      "este es el error en el paso i 61 15524.743\n",
      "este es el error en el paso i 62 15480.628\n",
      "este es el error en el paso i 63 15437.418\n",
      "este es el error en el paso i 64 15395.091\n",
      "este es el error en el paso i 65 15353.599\n",
      "este es el error en el paso i 66 15312.843\n",
      "este es el error en el paso i 67 15272.83\n",
      "este es el error en el paso i 68 15233.584\n",
      "este es el error en el paso i 69 15194.8955\n",
      "este es el error en el paso i 70 15156.859\n",
      "este es el error en el paso i 71 15119.441\n",
      "este es el error en el paso i 72 15082.598\n",
      "este es el error en el paso i 73 15046.252\n",
      "este es el error en el paso i 74 15010.466\n",
      "este es el error en el paso i 75 14975.141\n",
      "este es el error en el paso i 76 14940.263\n",
      "este es el error en el paso i 77 14905.867\n",
      "este es el error en el paso i 78 14871.853\n",
      "este es el error en el paso i 79 14838.255\n",
      "este es el error en el paso i 80 14805.023\n",
      "este es el error en el paso i 81 14772.225\n",
      "este es el error en el paso i 82 14739.699\n",
      "este es el error en el paso i 83 14707.57\n",
      "este es el error en el paso i 84 14675.737\n",
      "este es el error en el paso i 85 14644.236\n",
      "este es el error en el paso i 86 14613.025\n",
      "este es el error en el paso i 87 14582.061\n",
      "este es el error en el paso i 88 14551.408\n",
      "este es el error en el paso i 89 14521.091\n",
      "este es el error en el paso i 90 14490.857\n",
      "este es el error en el paso i 91 14461.03\n",
      "este es el error en el paso i 92 14431.392\n",
      "este es el error en el paso i 93 14401.979\n",
      "este es el error en el paso i 94 14372.744\n",
      "este es el error en el paso i 95 14343.774\n",
      "este es el error en el paso i 96 14314.984\n",
      "este es el error en el paso i 97 14286.438\n",
      "este es el error en el paso i 98 14258.053\n",
      "este es el error en el paso i 99 14229.833\n",
      "este es el error en el paso i 100 14201.813\n",
      "este es el error en el paso i 101 14173.986\n",
      "este es el error en el paso i 102 14146.279\n",
      "este es el error en el paso i 103 14118.782\n",
      "este es el error en el paso i 104 14091.449\n",
      "este es el error en el paso i 105 14064.265\n",
      "este es el error en el paso i 106 14037.214\n",
      "este es el error en el paso i 107 14010.297\n",
      "este es el error en el paso i 108 13983.567\n",
      "este es el error en el paso i 109 13956.943\n",
      "este es el error en el paso i 110 13930.466\n",
      "este es el error en el paso i 111 13904.141\n",
      "este es el error en el paso i 112 13877.912\n",
      "este es el error en el paso i 113 13851.852\n",
      "este es el error en el paso i 114 13825.87\n",
      "este es el error en el paso i 115 13800.074\n",
      "este es el error en el paso i 116 13774.311\n",
      "este es el error en el paso i 117 13748.721\n",
      "este es el error en el paso i 118 13723.224\n",
      "este es el error en el paso i 119 13697.842\n",
      "este es el error en el paso i 120 13672.596\n",
      "este es el error en el paso i 121 13647.409\n",
      "este es el error en el paso i 122 13622.358\n",
      "este es el error en el paso i 123 13597.375\n",
      "este es el error en el paso i 124 13572.506\n",
      "este es el error en el paso i 125 13547.78\n",
      "este es el error en el paso i 126 13523.103\n",
      "este es el error en el paso i 127 13498.593\n",
      "este es el error en el paso i 128 13474.114\n",
      "este es el error en el paso i 129 13449.757\n",
      "este es el error en el paso i 130 13425.4375\n",
      "este es el error en el paso i 131 13401.272\n",
      "este es el error en el paso i 132 13377.135\n",
      "este es el error en el paso i 133 13353.118\n",
      "este es el error en el paso i 134 13329.188\n",
      "este es el error en el paso i 135 13305.36\n",
      "este es el error en el paso i 136 13281.627\n",
      "este es el error en el paso i 137 13257.903\n",
      "este es el error en el paso i 138 13234.279\n",
      "este es el error en el paso i 139 13210.815\n",
      "este es el error en el paso i 140 13187.367\n",
      "este es el error en el paso i 141 13163.982\n",
      "este es el error en el paso i 142 13140.734\n",
      "este es el error en el paso i 143 13117.548\n",
      "este es el error en el paso i 144 13094.418\n",
      "este es el error en el paso i 145 13071.386\n",
      "este es el error en el paso i 146 13048.394\n",
      "este es el error en el paso i 147 13025.519\n",
      "este es el error en el paso i 148 13002.659\n",
      "este es el error en el paso i 149 12979.942\n",
      "este es el error en el paso i 150 12957.231\n",
      "este es el error en el paso i 151 12934.633\n",
      "este es el error en el paso i 152 12912.109\n",
      "este es el error en el paso i 153 12889.599\n",
      "este es el error en el paso i 154 12867.216\n",
      "este es el error en el paso i 155 12844.878\n",
      "este es el error en el paso i 156 12822.628\n",
      "este es el error en el paso i 157 12800.4\n",
      "este es el error en el paso i 158 12778.294\n",
      "este es el error en el paso i 159 12756.208\n",
      "este es el error en el paso i 160 12734.237\n",
      "este es el error en el paso i 161 12712.282\n",
      "este es el error en el paso i 162 12690.486\n",
      "este es el error en el paso i 163 12668.696\n",
      "este es el error en el paso i 164 12646.956\n",
      "este es el error en el paso i 165 12625.232\n",
      "este es el error en el paso i 166 12603.639\n",
      "este es el error en el paso i 167 12582.11\n",
      "este es el error en el paso i 168 12560.631\n",
      "este es el error en el paso i 169 12539.204\n",
      "este es el error en el paso i 170 12517.846\n",
      "este es el error en el paso i 171 12496.56\n",
      "este es el error en el paso i 172 12475.317\n",
      "este es el error en el paso i 173 12454.132\n",
      "este es el error en el paso i 174 12433.04\n",
      "este es el error en el paso i 175 12411.985\n",
      "este es el error en el paso i 176 12391.004\n",
      "este es el error en el paso i 177 12370.108\n",
      "este es el error en el paso i 178 12349.208\n",
      "este es el error en el paso i 179 12328.419\n",
      "este es el error en el paso i 180 12307.647\n",
      "este es el error en el paso i 181 12286.993\n",
      "este es el error en el paso i 182 12266.35\n",
      "este es el error en el paso i 183 12245.772\n",
      "este es el error en el paso i 184 12225.206\n",
      "este es el error en el paso i 185 12204.796\n",
      "este es el error en el paso i 186 12184.336\n",
      "este es el error en el paso i 187 12164.029\n",
      "este es el error en el paso i 188 12143.728\n",
      "este es el error en el paso i 189 12123.493\n",
      "este es el error en el paso i 190 12103.313\n",
      "este es el error en el paso i 191 12083.156\n",
      "este es el error en el paso i 192 12063.102\n",
      "este es el error en el paso i 193 12043.124\n",
      "este es el error en el paso i 194 12023.179\n",
      "este es el error en el paso i 195 12003.231\n",
      "este es el error en el paso i 196 11983.424\n",
      "este es el error en el paso i 197 11963.617\n",
      "este es el error en el paso i 198 11943.863\n",
      "este es el error en el paso i 199 11924.225\n",
      "este es el error en el paso i 200 11904.564\n",
      "este es el error en el paso i 201 11884.99\n",
      "este es el error en el paso i 202 11865.452\n",
      "este es el error en el paso i 203 11845.985\n",
      "este es el error en el paso i 204 11826.599\n",
      "este es el error en el paso i 205 11807.263\n",
      "este es el error en el paso i 206 11787.899\n",
      "este es el error en el paso i 207 11768.625\n",
      "este es el error en el paso i 208 11749.447\n",
      "este es el error en el paso i 209 11730.298\n",
      "este es el error en el paso i 210 11711.196\n",
      "este es el error en el paso i 211 11692.155\n",
      "este es el error en el paso i 212 11673.125\n",
      "este es el error en el paso i 213 11654.191\n",
      "este es el error en el paso i 214 11635.303\n",
      "este es el error en el paso i 215 11616.424\n",
      "este es el error en el paso i 216 11597.663\n",
      "este es el error en el paso i 217 11578.89\n",
      "este es el error en el paso i 218 11560.216\n",
      "este es el error en el paso i 219 11541.536\n",
      "este es el error en el paso i 220 11522.97\n",
      "este es el error en el paso i 221 11504.39\n",
      "este es el error en el paso i 222 11485.907\n",
      "este es el error en el paso i 223 11467.508\n",
      "este es el error en el paso i 224 11449.088\n",
      "este es el error en el paso i 225 11430.702\n",
      "este es el error en el paso i 226 11412.418\n",
      "este es el error en el paso i 227 11394.192\n",
      "este es el error en el paso i 228 11375.99\n",
      "este es el error en el paso i 229 11357.806\n",
      "este es el error en el paso i 230 11339.739\n",
      "este es el error en el paso i 231 11321.6455\n",
      "este es el error en el paso i 232 11303.649\n",
      "este es el error en el paso i 233 11285.699\n",
      "este es el error en el paso i 234 11267.799\n",
      "este es el error en el paso i 235 11249.924\n",
      "este es el error en el paso i 236 11232.109\n",
      "este es el error en el paso i 237 11214.359\n",
      "este es el error en el paso i 238 11196.601\n",
      "este es el error en el paso i 239 11178.92\n",
      "este es el error en el paso i 240 11161.306\n",
      "este es el error en el paso i 241 11143.706\n",
      "este es el error en el paso i 242 11126.172\n",
      "este es el error en el paso i 243 11108.684\n",
      "este es el error en el paso i 244 11091.215\n",
      "este es el error en el paso i 245 11073.835\n",
      "este es el error en el paso i 246 11056.484\n",
      "este es el error en el paso i 247 11039.173\n",
      "este es el error en el paso i 248 11021.9\n",
      "este es el error en el paso i 249 11004.687\n",
      "este es el error en el paso i 250 10987.532\n",
      "este es el error en el paso i 251 10970.384\n",
      "este es el error en el paso i 252 10953.303\n",
      "este es el error en el paso i 253 10936.277\n",
      "este es el error en el paso i 254 10919.282\n",
      "este es el error en el paso i 255 10902.346\n",
      "este es el error en el paso i 256 10885.416\n",
      "este es el error en el paso i 257 10868.599\n",
      "este es el error en el paso i 258 10851.721\n",
      "este es el error en el paso i 259 10834.963\n",
      "este es el error en el paso i 260 10818.215\n",
      "este es el error en el paso i 261 10801.564\n",
      "este es el error en el paso i 262 10784.93\n",
      "este es el error en el paso i 263 10768.306\n",
      "este es el error en el paso i 264 10751.747\n",
      "este es el error en el paso i 265 10735.258\n",
      "este es el error en el paso i 266 10718.735\n",
      "este es el error en el paso i 267 10702.344\n",
      "este es el error en el paso i 268 10685.9375\n",
      "este es el error en el paso i 269 10669.606\n",
      "este es el error en el paso i 270 10653.312\n",
      "este es el error en el paso i 271 10637.066\n",
      "este es el error en el paso i 272 10620.842\n",
      "este es el error en el paso i 273 10604.675\n",
      "este es el error en el paso i 274 10588.552\n",
      "este es el error en el paso i 275 10572.489\n",
      "este es el error en el paso i 276 10556.411\n",
      "este es el error en el paso i 277 10540.395\n",
      "este es el error en el paso i 278 10524.409\n",
      "este es el error en el paso i 279 10508.496\n",
      "este es el error en el paso i 280 10492.632\n",
      "este es el error en el paso i 281 10476.773\n",
      "este es el error en el paso i 282 10460.938\n",
      "este es el error en el paso i 283 10445.216\n",
      "este es el error en el paso i 284 10429.523\n",
      "este es el error en el paso i 285 10413.813\n",
      "este es el error en el paso i 286 10398.153\n",
      "este es el error en el paso i 287 10382.5625\n",
      "este es el error en el paso i 288 10366.992\n",
      "este es el error en el paso i 289 10351.468\n",
      "este es el error en el paso i 290 10336.007\n",
      "este es el error en el paso i 291 10320.538\n",
      "este es el error en el paso i 292 10305.155\n",
      "este es el error en el paso i 293 10289.788\n",
      "este es el error en el paso i 294 10274.465\n",
      "este es el error en el paso i 295 10259.21\n",
      "este es el error en el paso i 296 10243.954\n",
      "este es el error en el paso i 297 10228.721\n",
      "este es el error en el paso i 298 10213.543\n",
      "este es el error en el paso i 299 10198.428\n",
      "este es el error en el paso i 300 10183.335\n",
      "este es el error en el paso i 301 10168.302\n",
      "este es el error en el paso i 302 10153.273\n",
      "este es el error en el paso i 303 10138.304\n",
      "este es el error en el paso i 304 10123.365\n",
      "este es el error en el paso i 305 10108.485\n",
      "este es el error en el paso i 306 10093.6045\n",
      "este es el error en el paso i 307 10078.799\n",
      "este es el error en el paso i 308 10063.993\n",
      "este es el error en el paso i 309 10049.241\n",
      "este es el error en el paso i 310 10034.543\n",
      "este es el error en el paso i 311 10019.839\n",
      "este es el error en el paso i 312 10005.224\n",
      "este es el error en el paso i 313 9990.628\n",
      "este es el error en el paso i 314 9976.046\n",
      "este es el error en el paso i 315 9961.513\n",
      "este es el error en el paso i 316 9947.043\n",
      "este es el error en el paso i 317 9932.578\n",
      "este es el error en el paso i 318 9918.141\n",
      "este es el error en el paso i 319 9903.774\n",
      "este es el error en el paso i 320 9889.425\n",
      "este es el error en el paso i 321 9875.124\n",
      "este es el error en el paso i 322 9860.852\n",
      "este es el error en el paso i 323 9846.605\n",
      "este es el error en el paso i 324 9832.402\n",
      "este es el error en el paso i 325 9818.239\n",
      "este es el error en el paso i 326 9804.099\n",
      "este es el error en el paso i 327 9790.026\n",
      "este es el error en el paso i 328 9775.956\n",
      "este es el error en el paso i 329 9761.912\n",
      "este es el error en el paso i 330 9747.927\n",
      "este es el error en el paso i 331 9733.958\n",
      "este es el error en el paso i 332 9720.035\n",
      "este es el error en el paso i 333 9706.193\n",
      "este es el error en el paso i 334 9692.336\n",
      "este es el error en el paso i 335 9678.519\n",
      "este es el error en el paso i 336 9664.729\n",
      "este es el error en el paso i 337 9650.988\n",
      "este es el error en el paso i 338 9637.265\n",
      "este es el error en el paso i 339 9623.586\n",
      "este es el error en el paso i 340 9609.956\n",
      "este es el error en el paso i 341 9596.325\n",
      "este es el error en el paso i 342 9582.756\n",
      "este es el error en el paso i 343 9569.214\n",
      "este es el error en el paso i 344 9555.71\n",
      "este es el error en el paso i 345 9542.231\n",
      "este es el error en el paso i 346 9528.825\n",
      "este es el error en el paso i 347 9515.381\n",
      "este es el error en el paso i 348 9502.0205\n",
      "este es el error en el paso i 349 9488.654\n",
      "este es el error en el paso i 350 9475.346\n",
      "este es el error en el paso i 351 9462.05\n",
      "este es el error en el paso i 352 9448.822\n",
      "este es el error en el paso i 353 9435.598\n",
      "este es el error en el paso i 354 9422.414\n",
      "este es el error en el paso i 355 9409.298\n",
      "este es el error en el paso i 356 9396.18\n",
      "este es el error en el paso i 357 9383.074\n",
      "este es el error en el paso i 358 9370.021\n",
      "este es el error en el paso i 359 9357.026\n",
      "este es el error en el paso i 360 9344.023\n",
      "este es el error en el paso i 361 9331.103\n",
      "este es el error en el paso i 362 9318.191\n",
      "este es el error en el paso i 363 9305.275\n",
      "este es el error en el paso i 364 9292.405\n",
      "este es el error en el paso i 365 9279.571\n",
      "este es el error en el paso i 366 9266.769\n",
      "este es el error en el paso i 367 9254.023\n",
      "este es el error en el paso i 368 9241.282\n",
      "este es el error en el paso i 369 9228.596\n",
      "este es el error en el paso i 370 9215.922\n",
      "este es el error en el paso i 371 9203.271\n",
      "este es el error en el paso i 372 9190.67\n",
      "este es el error en el paso i 373 9178.113\n",
      "este es el error en el paso i 374 9165.536\n",
      "este es el error en el paso i 375 9153.015\n",
      "este es el error en el paso i 376 9140.54\n",
      "este es el error en el paso i 377 9128.09\n",
      "este es el error en el paso i 378 9115.652\n",
      "este es el error en el paso i 379 9103.262\n",
      "este es el error en el paso i 380 9090.908\n",
      "este es el error en el paso i 381 9078.563\n",
      "este es el error en el paso i 382 9066.246\n",
      "este es el error en el paso i 383 9053.979\n",
      "este es el error en el paso i 384 9041.737\n",
      "este es el error en el paso i 385 9029.494\n",
      "este es el error en el paso i 386 9017.308\n",
      "este es el error en el paso i 387 9005.184\n",
      "este es el error en el paso i 388 8993.06\n",
      "este es el error en el paso i 389 8980.943\n",
      "este es el error en el paso i 390 8968.9\n",
      "este es el error en el paso i 391 8956.841\n",
      "este es el error en el paso i 392 8944.817\n",
      "este es el error en el paso i 393 8932.841\n",
      "este es el error en el paso i 394 8920.893\n",
      "este es el error en el paso i 395 8908.955\n",
      "este es el error en el paso i 396 8897.079\n",
      "este es el error en el paso i 397 8885.195\n",
      "este es el error en el paso i 398 8873.336\n",
      "este es el error en el paso i 399 8861.573\n",
      "este es el error en el paso i 400 8849.788\n",
      "este es el error en el paso i 401 8838.027\n",
      "este es el error en el paso i 402 8826.296\n",
      "este es el error en el paso i 403 8814.602\n",
      "este es el error en el paso i 404 8802.95\n",
      "este es el error en el paso i 405 8791.29\n",
      "este es el error en el paso i 406 8779.681\n",
      "este es el error en el paso i 407 8768.111\n",
      "este es el error en el paso i 408 8756.533\n",
      "este es el error en el paso i 409 8745.0\n",
      "este es el error en el paso i 410 8733.492\n",
      "este es el error en el paso i 411 8722.043\n",
      "este es el error en el paso i 412 8710.609\n",
      "este es el error en el paso i 413 8699.161\n",
      "este es el error en el paso i 414 8687.784\n",
      "este es el error en el paso i 415 8676.401\n",
      "este es el error en el paso i 416 8665.086\n",
      "este es el error en el paso i 417 8653.77\n",
      "este es el error en el paso i 418 8642.476\n",
      "este es el error en el paso i 419 8631.218\n",
      "este es el error en el paso i 420 8619.994\n",
      "este es el error en el paso i 421 8608.783\n",
      "este es el error en el paso i 422 8597.6\n",
      "este es el error en el paso i 423 8586.439\n",
      "este es el error en el paso i 424 8575.323\n",
      "este es el error en el paso i 425 8564.225\n",
      "este es el error en el paso i 426 8553.171\n",
      "este es el error en el paso i 427 8542.098\n",
      "este es el error en el paso i 428 8531.082\n",
      "este es el error en el paso i 429 8520.084\n",
      "este es el error en el paso i 430 8509.126\n",
      "este es el error en el paso i 431 8498.176\n",
      "este es el error en el paso i 432 8487.2705\n",
      "este es el error en el paso i 433 8476.382\n",
      "este es el error en el paso i 434 8465.519\n",
      "este es el error en el paso i 435 8454.684\n",
      "este es el error en el paso i 436 8443.863\n",
      "este es el error en el paso i 437 8433.0625\n",
      "este es el error en el paso i 438 8422.315\n",
      "este es el error en el paso i 439 8411.595\n",
      "este es el error en el paso i 440 8400.871\n",
      "este es el error en el paso i 441 8390.173\n",
      "este es el error en el paso i 442 8379.515\n",
      "este es el error en el paso i 443 8368.902\n",
      "este es el error en el paso i 444 8358.264\n",
      "este es el error en el paso i 445 8347.681\n",
      "este es el error en el paso i 446 8337.108\n",
      "este es el error en el paso i 447 8326.593\n",
      "este es el error en el paso i 448 8316.06\n",
      "este es el error en el paso i 449 8305.594\n",
      "este es el error en el paso i 450 8295.102\n",
      "este es el error en el paso i 451 8284.665\n",
      "este es el error en el paso i 452 8274.253\n",
      "este es el error en el paso i 453 8263.859\n",
      "este es el error en el paso i 454 8253.498\n",
      "este es el error en el paso i 455 8243.172\n",
      "este es el error en el paso i 456 8232.846\n",
      "este es el error en el paso i 457 8222.548\n",
      "este es el error en el paso i 458 8212.277\n",
      "este es el error en el paso i 459 8202.033\n",
      "este es el error en el paso i 460 8191.8125\n",
      "este es el error en el paso i 461 8181.6143\n",
      "este es el error en el paso i 462 8171.427\n",
      "este es el error en el paso i 463 8161.2617\n",
      "este es el error en el paso i 464 8151.1655\n",
      "este es el error en el paso i 465 8141.048\n",
      "este es el error en el paso i 466 8130.955\n",
      "este es el error en el paso i 467 8120.901\n",
      "este es el error en el paso i 468 8110.8613\n",
      "este es el error en el paso i 469 8100.8516\n",
      "este es el error en el paso i 470 8090.8823\n",
      "este es el error en el paso i 471 8080.8843\n",
      "este es el error en el paso i 472 8070.954\n",
      "este es el error en el paso i 473 8061.0293\n",
      "este es el error en el paso i 474 8051.1475\n",
      "este es el error en el paso i 475 8041.247\n",
      "este es el error en el paso i 476 8031.413\n",
      "este es el error en el paso i 477 8021.573\n",
      "este es el error en el paso i 478 8011.7847\n",
      "este es el error en el paso i 479 8002.0005\n",
      "este es el error en el paso i 480 7992.2207\n",
      "este es el error en el paso i 481 7982.4863\n",
      "este es el error en el paso i 482 7972.7866\n",
      "este es el error en el paso i 483 7963.073\n",
      "este es el error en el paso i 484 7953.3926\n",
      "este es el error en el paso i 485 7943.736\n",
      "este es el error en el paso i 486 7934.132\n",
      "este es el error en el paso i 487 7924.5093\n",
      "este es el error en el paso i 488 7914.922\n",
      "este es el error en el paso i 489 7905.3784\n",
      "este es el error en el paso i 490 7895.835\n",
      "este es el error en el paso i 491 7886.3086\n",
      "este es el error en el paso i 492 7876.8164\n",
      "este es el error en el paso i 493 7867.3457\n",
      "este es el error en el paso i 494 7857.876\n",
      "este es el error en el paso i 495 7848.445\n",
      "este es el error en el paso i 496 7839.0454\n",
      "este es el error en el paso i 497 7829.637\n",
      "este es el error en el paso i 498 7820.2676\n",
      "este es el error en el paso i 499 7810.9204\n",
      "este es el error en el paso i 500 7801.6133\n",
      "este es el error en el paso i 501 7792.301\n",
      "este es el error en el paso i 502 7783.008\n",
      "este es el error en el paso i 503 7773.7373\n",
      "este es el error en el paso i 504 7764.5093\n",
      "este es el error en el paso i 505 7755.293\n",
      "este es el error en el paso i 506 7746.09\n",
      "este es el error en el paso i 507 7736.9263\n",
      "este es el error en el paso i 508 7727.7314\n",
      "este es el error en el paso i 509 7718.609\n",
      "este es el error en el paso i 510 7709.5005\n",
      "este es el error en el paso i 511 7700.3926\n",
      "este es el error en el paso i 512 7691.3066\n",
      "este es el error en el paso i 513 7682.2725\n",
      "este es el error en el paso i 514 7673.2285\n",
      "este es el error en el paso i 515 7664.211\n",
      "este es el error en el paso i 516 7655.214\n",
      "este es el error en el paso i 517 7646.2437\n",
      "este es el error en el paso i 518 7637.271\n",
      "este es el error en el paso i 519 7628.35\n",
      "este es el error en el paso i 520 7619.447\n",
      "este es el error en el paso i 521 7610.541\n",
      "este es el error en el paso i 522 7601.655\n",
      "este es el error en el paso i 523 7592.7925\n",
      "este es el error en el paso i 524 7583.966\n",
      "este es el error en el paso i 525 7575.151\n",
      "este es el error en el paso i 526 7566.375\n",
      "este es el error en el paso i 527 7557.5796\n",
      "este es el error en el paso i 528 7548.8145\n",
      "este es el error en el paso i 529 7540.0947\n",
      "este es el error en el paso i 530 7531.3594\n",
      "este es el error en el paso i 531 7522.665\n",
      "este es el error en el paso i 532 7513.9824\n",
      "este es el error en el paso i 533 7505.3223\n",
      "este es el error en el paso i 534 7496.7007\n",
      "este es el error en el paso i 535 7488.067\n",
      "este es el error en el paso i 536 7479.4644\n",
      "este es el error en el paso i 537 7470.869\n",
      "este es el error en el paso i 538 7462.302\n",
      "este es el error en el paso i 539 7453.7773\n",
      "este es el error en el paso i 540 7445.2427\n",
      "este es el error en el paso i 541 7436.7363\n",
      "este es el error en el paso i 542 7428.235\n",
      "este es el error en el paso i 543 7419.7676\n",
      "este es el error en el paso i 544 7411.326\n",
      "este es el error en el paso i 545 7402.889\n",
      "este es el error en el paso i 546 7394.453\n",
      "este es el error en el paso i 547 7386.0723\n",
      "este es el error en el paso i 548 7377.6973\n",
      "este es el error en el paso i 549 7369.3335\n",
      "este es el error en el paso i 550 7360.9834\n",
      "este es el error en el paso i 551 7352.692\n",
      "este es el error en el paso i 552 7344.3687\n",
      "este es el error en el paso i 553 7336.0664\n",
      "este es el error en el paso i 554 7327.802\n",
      "este es el error en el paso i 555 7319.5728\n",
      "este es el error en el paso i 556 7311.3574\n",
      "este es el error en el paso i 557 7303.131\n",
      "este es el error en el paso i 558 7294.9214\n",
      "este es el error en el paso i 559 7286.7344\n",
      "este es el error en el paso i 560 7278.5757\n",
      "este es el error en el paso i 561 7270.4507\n",
      "este es el error en el paso i 562 7262.314\n",
      "este es el error en el paso i 563 7254.21\n",
      "este es el error en el paso i 564 7246.0967\n",
      "este es el error en el paso i 565 7238.039\n",
      "este es el error en el paso i 566 7230.012\n",
      "este es el error en el paso i 567 7221.9766\n",
      "este es el error en el paso i 568 7213.9385\n",
      "este es el error en el paso i 569 7205.9346\n",
      "este es el error en el paso i 570 7197.9507\n",
      "este es el error en el paso i 571 7189.9785\n",
      "este es el error en el paso i 572 7182.039\n",
      "este es el error en el paso i 573 7174.0913\n",
      "este es el error en el paso i 574 7166.194\n",
      "este es el error en el paso i 575 7158.292\n",
      "este es el error en el paso i 576 7150.4224\n",
      "este es el error en el paso i 577 7142.531\n",
      "este es el error en el paso i 578 7134.701\n",
      "este es el error en el paso i 579 7126.87\n",
      "este es el error en el paso i 580 7119.0566\n",
      "este es el error en el paso i 581 7111.273\n",
      "este es el error en el paso i 582 7103.488\n",
      "este es el error en el paso i 583 7095.7363\n",
      "este es el error en el paso i 584 7087.994\n",
      "este es el error en el paso i 585 7080.241\n",
      "este es el error en el paso i 586 7072.5347\n",
      "este es el error en el paso i 587 7064.852\n",
      "este es el error en el paso i 588 7057.147\n",
      "este es el error en el paso i 589 7049.492\n",
      "este es el error en el paso i 590 7041.8594\n",
      "este es el error en el paso i 591 7034.234\n",
      "este es el error en el paso i 592 7026.587\n",
      "este es el error en el paso i 593 7019.016\n",
      "este es el error en el paso i 594 7011.4336\n",
      "este es el error en el paso i 595 7003.887\n",
      "este es el error en el paso i 596 6996.3335\n",
      "este es el error en el paso i 597 6988.7876\n",
      "este es el error en el paso i 598 6981.2593\n",
      "este es el error en el paso i 599 6973.7656\n",
      "este es el error en el paso i 600 6966.277\n",
      "este es el error en el paso i 601 6958.8257\n",
      "este es el error en el paso i 602 6951.354\n",
      "este es el error en el paso i 603 6943.9355\n",
      "este es el error en el paso i 604 6936.4946\n",
      "este es el error en el paso i 605 6929.1016\n",
      "este es el error en el paso i 606 6921.71\n",
      "este es el error en el paso i 607 6914.314\n",
      "este es el error en el paso i 608 6906.9727\n",
      "este es el error en el paso i 609 6899.6187\n",
      "este es el error en el paso i 610 6892.3022\n",
      "este es el error en el paso i 611 6884.998\n",
      "este es el error en el paso i 612 6877.718\n",
      "este es el error en el paso i 613 6870.4204\n",
      "este es el error en el paso i 614 6863.1675\n",
      "este es el error en el paso i 615 6855.9067\n",
      "este es el error en el paso i 616 6848.6733\n",
      "este es el error en el paso i 617 6841.4688\n",
      "este es el error en el paso i 618 6834.2285\n",
      "este es el error en el paso i 619 6827.0654\n",
      "este es el error en el paso i 620 6819.8594\n",
      "este es el error en el paso i 621 6812.7197\n",
      "este es el error en el paso i 622 6805.572\n",
      "este es el error en el paso i 623 6798.4307\n",
      "este es el error en el paso i 624 6791.345\n",
      "este es el error en el paso i 625 6784.234\n",
      "este es el error en el paso i 626 6777.1504\n",
      "este es el error en el paso i 627 6770.081\n",
      "este es el error en el paso i 628 6763.033\n",
      "este es el error en el paso i 629 6755.997\n",
      "este es el error en el paso i 630 6748.966\n",
      "este es el error en el paso i 631 6741.9634\n",
      "este es el error en el paso i 632 6734.96\n",
      "este es el error en el paso i 633 6727.97\n",
      "este es el error en el paso i 634 6721.023\n",
      "este es el error en el paso i 635 6714.0713\n",
      "este es el error en el paso i 636 6707.125\n",
      "este es el error en el paso i 637 6700.2007\n",
      "este es el error en el paso i 638 6693.306\n",
      "este es el error en el paso i 639 6686.3887\n",
      "este es el error en el paso i 640 6679.5127\n",
      "este es el error en el paso i 641 6672.641\n",
      "este es el error en el paso i 642 6665.8066\n",
      "este es el error en el paso i 643 6658.966\n",
      "este es el error en el paso i 644 6652.136\n",
      "este es el error en el paso i 645 6645.3296\n",
      "este es el error en el paso i 646 6638.537\n",
      "este es el error en el paso i 647 6631.7583\n",
      "este es el error en el paso i 648 6624.99\n",
      "este es el error en el paso i 649 6618.2393\n",
      "este es el error en el paso i 650 6611.4966\n",
      "este es el error en el paso i 651 6604.7715\n",
      "este es el error en el paso i 652 6598.0483\n",
      "este es el error en el paso i 653 6591.3604\n",
      "este es el error en el paso i 654 6584.682\n",
      "este es el error en el paso i 655 6577.9946\n",
      "este es el error en el paso i 656 6571.358\n",
      "este es el error en el paso i 657 6564.717\n",
      "este es el error en el paso i 658 6558.0835\n",
      "este es el error en el paso i 659 6551.4653\n",
      "este es el error en el paso i 660 6544.868\n",
      "este es el error en el paso i 661 6538.277\n",
      "este es el error en el paso i 662 6531.692\n",
      "este es el error en el paso i 663 6525.1245\n",
      "este es el error en el paso i 664 6518.592\n",
      "este es el error en el paso i 665 6512.0625\n",
      "este es el error en el paso i 666 6505.53\n",
      "este es el error en el paso i 667 6499.029\n",
      "este es el error en el paso i 668 6492.5405\n",
      "este es el error en el paso i 669 6486.061\n",
      "este es el error en el paso i 670 6479.598\n",
      "este es el error en el paso i 671 6473.14\n",
      "este es el error en el paso i 672 6466.6826\n",
      "este es el error en el paso i 673 6460.2764\n",
      "este es el error en el paso i 674 6453.86\n",
      "este es el error en el paso i 675 6447.452\n",
      "este es el error en el paso i 676 6441.062\n",
      "este es el error en el paso i 677 6434.663\n",
      "este es el error en el paso i 678 6428.3257\n",
      "este es el error en el paso i 679 6421.969\n",
      "este es el error en el paso i 680 6415.628\n",
      "este es el error en el paso i 681 6409.301\n",
      "este es el error en el paso i 682 6403.011\n",
      "este es el error en el paso i 683 6396.6914\n",
      "este es el error en el paso i 684 6390.409\n",
      "este es el error en el paso i 685 6384.1494\n",
      "este es el error en el paso i 686 6377.87\n",
      "este es el error en el paso i 687 6371.6187\n",
      "este es el error en el paso i 688 6365.3877\n",
      "este es el error en el paso i 689 6359.1777\n",
      "este es el error en el paso i 690 6352.9585\n",
      "este es el error en el paso i 691 6346.745\n",
      "este es el error en el paso i 692 6340.574\n",
      "este es el error en el paso i 693 6334.3877\n",
      "este es el error en el paso i 694 6328.2285\n",
      "este es el error en el paso i 695 6322.075\n",
      "este es el error en el paso i 696 6315.941\n",
      "este es el error en el paso i 697 6309.8276\n",
      "este es el error en el paso i 698 6303.732\n",
      "este es el error en el paso i 699 6297.6177\n",
      "este es el error en el paso i 700 6291.526\n",
      "este es el error en el paso i 701 6285.4526\n",
      "este es el error en el paso i 702 6279.381\n",
      "este es el error en el paso i 703 6273.335\n",
      "este es el error en el paso i 704 6267.3003\n",
      "este es el error en el paso i 705 6261.282\n",
      "este es el error en el paso i 706 6255.2734\n",
      "este es el error en el paso i 707 6249.25\n",
      "este es el error en el paso i 708 6243.2686\n",
      "este es el error en el paso i 709 6237.282\n",
      "este es el error en el paso i 710 6231.327\n",
      "este es el error en el paso i 711 6225.3564\n",
      "este es el error en el paso i 712 6219.4146\n",
      "este es el error en el paso i 713 6213.491\n",
      "este es el error en el paso i 714 6207.5664\n",
      "este es el error en el paso i 715 6201.662\n",
      "este es el error en el paso i 716 6195.7446\n",
      "este es el error en el paso i 717 6189.8887\n",
      "este es el error en el paso i 718 6184.025\n",
      "este es el error en el paso i 719 6178.1367\n",
      "este es el error en el paso i 720 6172.3037\n",
      "este es el error en el paso i 721 6166.4634\n",
      "este es el error en el paso i 722 6160.6206\n",
      "este es el error en el paso i 723 6154.8223\n",
      "este es el error en el paso i 724 6149.0176\n",
      "este es el error en el paso i 725 6143.217\n",
      "este es el error en el paso i 726 6137.436\n",
      "este es el error en el paso i 727 6131.6567\n",
      "este es el error en el paso i 728 6125.8975\n",
      "este es el error en el paso i 729 6120.169\n",
      "este es el error en el paso i 730 6114.4136\n",
      "este es el error en el paso i 731 6108.6934\n",
      "este es el error en el paso i 732 6102.986\n",
      "este es el error en el paso i 733 6097.2817\n",
      "este es el error en el paso i 734 6091.5776\n",
      "este es el error en el paso i 735 6085.9136\n",
      "este es el error en el paso i 736 6080.2427\n",
      "este es el error en el paso i 737 6074.594\n",
      "este es el error en el paso i 738 6068.929\n",
      "este es el error en el paso i 739 6063.298\n",
      "este es el error en el paso i 740 6057.684\n",
      "este es el error en el paso i 741 6052.079\n",
      "este es el error en el paso i 742 6046.4644\n",
      "este es el error en el paso i 743 6040.8735\n",
      "este es el error en el paso i 744 6035.2817\n",
      "este es el error en el paso i 745 6029.7197\n",
      "este es el error en el paso i 746 6024.1606\n",
      "este es el error en el paso i 747 6018.6157\n",
      "este es el error en el paso i 748 6013.077\n",
      "este es el error en el paso i 749 6007.5464\n",
      "este es el error en el paso i 750 6002.0215\n",
      "este es el error en el paso i 751 5996.528\n",
      "este es el error en el paso i 752 5991.032\n",
      "este es el error en el paso i 753 5985.549\n",
      "este es el error en el paso i 754 5980.054\n",
      "este es el error en el paso i 755 5974.5996\n",
      "este es el error en el paso i 756 5969.1484\n",
      "este es el error en el paso i 757 5963.7207\n",
      "este es el error en el paso i 758 5958.2925\n",
      "este es el error en el paso i 759 5952.845\n",
      "este es el error en el paso i 760 5947.4365\n",
      "este es el error en el paso i 761 5942.048\n",
      "este es el error en el paso i 762 5936.657\n",
      "este es el error en el paso i 763 5931.2627\n",
      "este es el error en el paso i 764 5925.8945\n",
      "este es el error en el paso i 765 5920.5483\n",
      "este es el error en el paso i 766 5915.189\n",
      "este es el error en el paso i 767 5909.863\n",
      "este es el error en el paso i 768 5904.5327\n",
      "este es el error en el paso i 769 5899.213\n",
      "este es el error en el paso i 770 5893.9043\n",
      "este es el error en el paso i 771 5888.608\n",
      "este es el error en el paso i 772 5883.3047\n",
      "este es el error en el paso i 773 5878.0415\n",
      "este es el error en el paso i 774 5872.761\n",
      "este es el error en el paso i 775 5867.507\n",
      "este es el error en el paso i 776 5862.2705\n",
      "este es el error en el paso i 777 5857.042\n",
      "este es el error en el paso i 778 5851.796\n",
      "este es el error en el paso i 779 5846.577\n",
      "este es el error en el paso i 780 5841.3633\n",
      "este es el error en el paso i 781 5836.1714\n",
      "este es el error en el paso i 782 5830.972\n",
      "este es el error en el paso i 783 5825.808\n",
      "este es el error en el paso i 784 5820.6387\n",
      "este es el error en el paso i 785 5815.4795\n",
      "este es el error en el paso i 786 5810.314\n",
      "este es el error en el paso i 787 5805.1963\n",
      "este es el error en el paso i 788 5800.0664\n",
      "este es el error en el paso i 789 5794.931\n",
      "este es el error en el paso i 790 5789.8325\n",
      "este es el error en el paso i 791 5784.727\n",
      "este es el error en el paso i 792 5779.6387\n",
      "este es el error en el paso i 793 5774.5435\n",
      "este es el error en el paso i 794 5769.4883\n",
      "este es el error en el paso i 795 5764.42\n",
      "este es el error en el paso i 796 5759.3755\n",
      "este es el error en el paso i 797 5754.3237\n",
      "este es el error en el paso i 798 5749.2827\n",
      "este es el error en el paso i 799 5744.265\n",
      "este es el error en el paso i 800 5739.2383\n",
      "este es el error en el paso i 801 5734.2407\n",
      "este es el error en el paso i 802 5729.245\n",
      "este es el error en el paso i 803 5724.2627\n",
      "este es el error en el paso i 804 5719.291\n",
      "este es el error en el paso i 805 5714.301\n",
      "este es el error en el paso i 806 5709.348\n",
      "este es el error en el paso i 807 5704.3867\n",
      "este es el error en el paso i 808 5699.453\n",
      "este es el error en el paso i 809 5694.533\n",
      "este es el error en el paso i 810 5689.5996\n",
      "este es el error en el paso i 811 5684.68\n",
      "este es el error en el paso i 812 5679.79\n",
      "este es el error en el paso i 813 5674.892\n",
      "este es el error en el paso i 814 5670.0303\n",
      "este es el error en el paso i 815 5665.123\n",
      "este es el error en el paso i 816 5660.2593\n",
      "este es el error en el paso i 817 5655.399\n",
      "este es el error en el paso i 818 5650.555\n",
      "este es el error en el paso i 819 5645.723\n",
      "este es el error en el paso i 820 5640.8867\n",
      "este es el error en el paso i 821 5636.0703\n",
      "este es el error en el paso i 822 5631.24\n",
      "este es el error en el paso i 823 5626.436\n",
      "este es el error en el paso i 824 5621.6353\n",
      "este es el error en el paso i 825 5616.839\n",
      "este es el error en el paso i 826 5612.0664\n",
      "este es el error en el paso i 827 5607.309\n",
      "este es el error en el paso i 828 5602.5264\n",
      "este es el error en el paso i 829 5597.7837\n",
      "este es el error en el paso i 830 5593.056\n",
      "este es el error en el paso i 831 5588.302\n",
      "este es el error en el paso i 832 5583.572\n",
      "este es el error en el paso i 833 5578.86\n",
      "este es el error en el paso i 834 5574.1606\n",
      "este es el error en el paso i 835 5569.4575\n",
      "este es el error en el paso i 836 5564.768\n",
      "este es el error en el paso i 837 5560.0713\n",
      "este es el error en el paso i 838 5555.3906\n",
      "este es el error en el paso i 839 5550.735\n",
      "este es el error en el paso i 840 5546.08\n",
      "este es el error en el paso i 841 5541.4453\n",
      "este es el error en el paso i 842 5536.793\n",
      "este es el error en el paso i 843 5532.1665\n",
      "este es el error en el paso i 844 5527.53\n",
      "este es el error en el paso i 845 5522.924\n",
      "este es el error en el paso i 846 5518.32\n",
      "este es el error en el paso i 847 5513.7173\n",
      "este es el error en el paso i 848 5509.1343\n",
      "este es el error en el paso i 849 5504.543\n",
      "este es el error en el paso i 850 5499.963\n",
      "este es el error en el paso i 851 5495.4053\n",
      "este es el error en el paso i 852 5490.8506\n",
      "este es el error en el paso i 853 5486.2944\n",
      "este es el error en el paso i 854 5481.7603\n",
      "este es el error en el paso i 855 5477.2197\n",
      "este es el error en el paso i 856 5472.7095\n",
      "este es el error en el paso i 857 5468.188\n",
      "este es el error en el paso i 858 5463.6865\n",
      "este es el error en el paso i 859 5459.1724\n",
      "este es el error en el paso i 860 5454.683\n",
      "este es el error en el paso i 861 5450.1953\n",
      "este es el error en el paso i 862 5445.7393\n",
      "este es el error en el paso i 863 5441.249\n",
      "este es el error en el paso i 864 5436.801\n",
      "este es el error en el paso i 865 5432.336\n",
      "este es el error en el paso i 866 5427.8955\n",
      "este es el error en el paso i 867 5423.468\n",
      "este es el error en el paso i 868 5419.0444\n",
      "este es el error en el paso i 869 5414.6206\n",
      "este es el error en el paso i 870 5410.22\n",
      "este es el error en el paso i 871 5405.8135\n",
      "este es el error en el paso i 872 5401.3975\n",
      "este es el error en el paso i 873 5397.0273\n",
      "este es el error en el paso i 874 5392.644\n",
      "este es el error en el paso i 875 5388.259\n",
      "este es el error en el paso i 876 5383.912\n",
      "este es el error en el paso i 877 5379.537\n",
      "este es el error en el paso i 878 5375.1836\n",
      "este es el error en el paso i 879 5370.85\n",
      "este es el error en el paso i 880 5366.5205\n",
      "este es el error en el paso i 881 5362.1978\n",
      "este es el error en el paso i 882 5357.8716\n",
      "este es el error en el paso i 883 5353.5625\n",
      "este es el error en el paso i 884 5349.262\n",
      "este es el error en el paso i 885 5344.962\n",
      "este es el error en el paso i 886 5340.6694\n",
      "este es el error en el paso i 887 5336.3994\n",
      "este es el error en el paso i 888 5332.136\n",
      "este es el error en el paso i 889 5327.861\n",
      "este es el error en el paso i 890 5323.611\n",
      "este es el error en el paso i 891 5319.35\n",
      "este es el error en el paso i 892 5315.108\n",
      "este es el error en el paso i 893 5310.878\n",
      "este es el error en el paso i 894 5306.649\n",
      "este es el error en el paso i 895 5302.4272\n",
      "este es el error en el paso i 896 5298.203\n",
      "este es el error en el paso i 897 5294.0034\n",
      "este es el error en el paso i 898 5289.7974\n",
      "este es el error en el paso i 899 5285.6294\n",
      "este es el error en el paso i 900 5281.443\n",
      "este es el error en el paso i 901 5277.2656\n",
      "este es el error en el paso i 902 5273.096\n",
      "este es el error en el paso i 903 5268.933\n",
      "este es el error en el paso i 904 5264.784\n",
      "este es el error en el paso i 905 5260.6357\n",
      "este es el error en el paso i 906 5256.474\n",
      "este es el error en el paso i 907 5252.355\n",
      "este es el error en el paso i 908 5248.227\n",
      "este es el error en el paso i 909 5244.1123\n",
      "este es el error en el paso i 910 5239.998\n",
      "este es el error en el paso i 911 5235.896\n",
      "este es el error en el paso i 912 5231.7983\n",
      "este es el error en el paso i 913 5227.719\n",
      "este es el error en el paso i 914 5223.6235\n",
      "este es el error en el paso i 915 5219.5474\n",
      "este es el error en el paso i 916 5215.4873\n",
      "este es el error en el paso i 917 5211.425\n",
      "este es el error en el paso i 918 5207.3613\n",
      "este es el error en el paso i 919 5203.311\n",
      "este es el error en el paso i 920 5199.26\n",
      "este es el error en el paso i 921 5195.241\n",
      "este es el error en el paso i 922 5191.208\n",
      "este es el error en el paso i 923 5187.1914\n",
      "este es el error en el paso i 924 5183.182\n",
      "este es el error en el paso i 925 5179.168\n",
      "este es el error en el paso i 926 5175.1904\n",
      "este es el error en el paso i 927 5171.19\n",
      "este es el error en el paso i 928 5167.2095\n",
      "este es el error en el paso i 929 5163.231\n",
      "este es el error en el paso i 930 5159.268\n",
      "este es el error en el paso i 931 5155.293\n",
      "este es el error en el paso i 932 5151.3276\n",
      "este es el error en el paso i 933 5147.385\n",
      "este es el error en el paso i 934 5143.446\n",
      "este es el error en el paso i 935 5139.5063\n",
      "este es el error en el paso i 936 5135.561\n",
      "este es el error en el paso i 937 5131.628\n",
      "este es el error en el paso i 938 5127.7085\n",
      "este es el error en el paso i 939 5123.826\n",
      "este es el error en el paso i 940 5119.9106\n",
      "este es el error en el paso i 941 5116.0093\n",
      "este es el error en el paso i 942 5112.13\n",
      "este es el error en el paso i 943 5108.243\n",
      "este es el error en el paso i 944 5104.3516\n",
      "este es el error en el paso i 945 5100.4893\n",
      "este es el error en el paso i 946 5096.6265\n",
      "este es el error en el paso i 947 5092.764\n",
      "este es el error en el paso i 948 5088.9067\n",
      "este es el error en el paso i 949 5085.065\n",
      "este es el error en el paso i 950 5081.2393\n",
      "este es el error en el paso i 951 5077.398\n",
      "este es el error en el paso i 952 5073.5796\n",
      "este es el error en el paso i 953 5069.754\n",
      "este es el error en el paso i 954 5065.947\n",
      "este es el error en el paso i 955 5062.1577\n",
      "este es el error en el paso i 956 5058.3438\n",
      "este es el error en el paso i 957 5054.546\n",
      "este es el error en el paso i 958 5050.775\n",
      "este es el error en el paso i 959 5046.983\n",
      "este es el error en el paso i 960 5043.207\n",
      "este es el error en el paso i 961 5039.4365\n",
      "este es el error en el paso i 962 5035.6665\n",
      "este es el error en el paso i 963 5031.9243\n",
      "este es el error en el paso i 964 5028.169\n",
      "este es el error en el paso i 965 5024.431\n",
      "este es el error en el paso i 966 5020.692\n",
      "este es el error en el paso i 967 5016.966\n",
      "este es el error en el paso i 968 5013.2334\n",
      "este es el error en el paso i 969 5009.5254\n",
      "este es el error en el paso i 970 5005.813\n",
      "este es el error en el paso i 971 5002.1055\n",
      "este es el error en el paso i 972 4998.3936\n",
      "este es el error en el paso i 973 4994.729\n",
      "este es el error en el paso i 974 4991.038\n",
      "este es el error en el paso i 975 4987.354\n",
      "este es el error en el paso i 976 4983.685\n",
      "este es el error en el paso i 977 4980.0103\n",
      "este es el error en el paso i 978 4976.351\n",
      "este es el error en el paso i 979 4972.6987\n",
      "este es el error en el paso i 980 4969.0454\n",
      "este es el error en el paso i 981 4965.413\n",
      "este es el error en el paso i 982 4961.7603\n",
      "este es el error en el paso i 983 4958.138\n",
      "este es el error en el paso i 984 4954.5063\n",
      "este es el error en el paso i 985 4950.8887\n",
      "este es el error en el paso i 986 4947.277\n",
      "este es el error en el paso i 987 4943.6636\n",
      "este es el error en el paso i 988 4940.057\n",
      "este es el error en el paso i 989 4936.4697\n",
      "este es el error en el paso i 990 4932.89\n",
      "este es el error en el paso i 991 4929.316\n",
      "este es el error en el paso i 992 4925.725\n",
      "este es el error en el paso i 993 4922.1616\n",
      "este es el error en el paso i 994 4918.5933\n",
      "este es el error en el paso i 995 4915.031\n",
      "este es el error en el paso i 996 4911.4746\n",
      "este es el error en el paso i 997 4907.914\n",
      "este es el error en el paso i 998 4904.3774\n",
      "este es el error en el paso i 999 4900.817\n",
      "este es theta [[-0.7678599 ]\n",
      " [ 0.3799932 ]\n",
      " [ 1.9103469 ]\n",
      " [-0.05590359]\n",
      " [-0.8859426 ]\n",
      " [ 0.64356315]\n",
      " [ 0.47674805]\n",
      " [-1.4059972 ]\n",
      " [-0.00546864]\n",
      " [ 0.42290866]\n",
      " [-0.2553109 ]\n",
      " [-0.5828618 ]\n",
      " [ 0.06700674]\n",
      " [-0.14449222]\n",
      " [ 0.7732873 ]\n",
      " [ 0.2609355 ]\n",
      " [ 1.9275522 ]\n",
      " [ 0.56928414]\n",
      " [-1.553668  ]\n",
      " [-1.4667703 ]\n",
      " [ 0.1897064 ]\n",
      " [-1.8406044 ]\n",
      " [-0.48533988]\n",
      " [ 0.28478536]\n",
      " [ 0.3273741 ]\n",
      " [-0.73392737]\n",
      " [ 0.43530095]\n",
      " [-0.3087891 ]\n",
      " [-0.48725858]\n",
      " [-0.17354612]\n",
      " [ 0.04387243]\n",
      " [-0.01951441]\n",
      " [ 0.9185937 ]\n",
      " [-0.574891  ]\n",
      " [ 0.44314465]\n",
      " [ 0.12001443]\n",
      " [-0.24388598]\n",
      " [-0.3731331 ]\n",
      " [ 0.0988429 ]\n",
      " [ 0.30070812]\n",
      " [-0.8556681 ]\n",
      " [-0.34121662]\n",
      " [-0.42082614]\n",
      " [-1.2293109 ]\n",
      " [-0.9277087 ]\n",
      " [-0.02702549]\n",
      " [-0.57323915]\n",
      " [ 0.323632  ]\n",
      " [-0.3208862 ]\n",
      " [ 2.0783744 ]\n",
      " [-1.1045502 ]\n",
      " [ 0.33466598]\n",
      " [-0.18862239]\n",
      " [ 0.03290519]\n",
      " [-1.0252649 ]\n",
      " [-0.2984581 ]\n",
      " [-0.33771026]\n",
      " [-1.1329881 ]\n",
      " [ 0.31853345]\n",
      " [ 0.8563684 ]\n",
      " [ 0.21375804]\n",
      " [ 0.29305443]\n",
      " [ 1.1773266 ]\n",
      " [ 0.28290647]\n",
      " [-0.36809307]\n",
      " [ 1.439923  ]\n",
      " [ 0.3078876 ]\n",
      " [ 0.29174256]\n",
      " [ 0.29913917]\n",
      " [ 0.5878727 ]\n",
      " [-0.6294687 ]\n",
      " [-1.3250175 ]\n",
      " [ 1.1834898 ]\n",
      " [ 0.5184508 ]\n",
      " [-0.31492835]\n",
      " [-1.6194948 ]\n",
      " [-0.4713596 ]\n",
      " [-0.02625108]\n",
      " [-0.90872407]\n",
      " [ 0.47639605]\n",
      " [-0.4512323 ]\n",
      " [ 0.37835747]\n",
      " [-0.83411926]\n",
      " [-2.2131667 ]\n",
      " [ 0.2439599 ]\n",
      " [-0.31016758]\n",
      " [-0.92274415]\n",
      " [-0.93386286]\n",
      " [ 0.12179059]\n",
      " [ 0.19762662]\n",
      " [ 0.3958472 ]]\n",
      "este es y_hat de training [[ 0.9178071 ]\n",
      " [-0.04150665]\n",
      " [ 0.4835472 ]\n",
      " ...\n",
      " [-0.26327968]\n",
      " [ 0.17122377]\n",
      " [-0.507797  ]]\n",
      "este es y_hat de val [[ 0.82691187]\n",
      " [-0.0931989 ]\n",
      " [-0.45302212]\n",
      " ...\n",
      " [-0.3508159 ]\n",
      " [ 0.06955391]\n",
      " [-0.35012162]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m run_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlineal\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m y_hatl,y_hat_vall\u001b[39m=\u001b[39mmodelo\u001b[39m.\u001b[39;49mmodel(theta, x_trainl, y_trainl, lr,n_steps,x_vl,y_vl,run_name)\n",
      "File \u001b[1;32mc:\\Users\\52333\\Documents\\doctorado\\ml-md\\proyecto\\linear.py:173\u001b[0m, in \u001b[0;36mLinear_Model.model\u001b[1;34m(self, theta, X, y, lr, n_steps, X_val, y_val, run_name)\u001b[0m\n\u001b[0;32m    171\u001b[0m y_hat_val\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimate_grsl(X_val,theta)\n\u001b[0;32m    172\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39meste es y_hat de val\u001b[39m\u001b[39m'\u001b[39m,y_hat_val)\n\u001b[1;32m--> 173\u001b[0m precision,accuracy\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision(y,y_hat),\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccuracy(y,y_hat)\n\u001b[0;32m    175\u001b[0m precision_val,accuracy_val\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision(y_val,y_hat_val),\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccuracy(y_val,y_hat_val)\n\u001b[0;32m    176\u001b[0m mlflow\u001b[39m.\u001b[39mlog_metric(\u001b[39m'\u001b[39m\u001b[39mprecision\u001b[39m\u001b[39m'\u001b[39m,precision)\n",
      "File \u001b[1;32mc:\\Users\\52333\\Documents\\doctorado\\ml-md\\proyecto\\linear.py:126\u001b[0m, in \u001b[0;36mLinear_Model.accuracy\u001b[1;34m(self, y, y_hat)\u001b[0m\n\u001b[0;32m    124\u001b[0m FN\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[0;32m    125\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(y)):\n\u001b[1;32m--> 126\u001b[0m     \u001b[39mif\u001b[39;00m(y_hat[i]\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m y[i]\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m) :\n\u001b[0;32m    127\u001b[0m         TP\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m    128\u001b[0m     \u001b[39mif\u001b[39;00m(y_hat[i]\u001b[39m<\u001b[39m\u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m y[i]\u001b[39m<\u001b[39m\u001b[39m0\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\52333\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:3570\u001b[0m, in \u001b[0;36m_rewriting_take\u001b[1;34m(arr, idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[0;32m   3567\u001b[0m   \u001b[39mreturn\u001b[39;00m _getslice(arr, start, stop)\n\u001b[0;32m   3569\u001b[0m treedef, static_idx, dynamic_idx \u001b[39m=\u001b[39m _split_index_for_jit(idx, arr\u001b[39m.\u001b[39mshape)\n\u001b[1;32m-> 3570\u001b[0m \u001b[39mreturn\u001b[39;00m _gather(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n\u001b[0;32m   3571\u001b[0m                unique_indices, mode, fill_value)\n",
      "File \u001b[1;32mc:\\Users\\52333\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:3579\u001b[0m, in \u001b[0;36m_gather\u001b[1;34m(arr, treedef, static_idx, dynamic_idx, indices_are_sorted, unique_indices, mode, fill_value)\u001b[0m\n\u001b[0;32m   3576\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_gather\u001b[39m(arr, treedef, static_idx, dynamic_idx, indices_are_sorted,\n\u001b[0;32m   3577\u001b[0m             unique_indices, mode, fill_value):\n\u001b[0;32m   3578\u001b[0m   idx \u001b[39m=\u001b[39m _merge_static_and_dynamic_indices(treedef, static_idx, dynamic_idx)\n\u001b[1;32m-> 3579\u001b[0m   indexer \u001b[39m=\u001b[39m _index_to_gather(shape(arr), idx)  \u001b[39m# shared with _scatter_update\u001b[39;00m\n\u001b[0;32m   3580\u001b[0m   y \u001b[39m=\u001b[39m arr\n\u001b[0;32m   3582\u001b[0m   \u001b[39mif\u001b[39;00m fill_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\52333\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:3779\u001b[0m, in \u001b[0;36m_index_to_gather\u001b[1;34m(x_shape, idx, normalize_indices)\u001b[0m\n\u001b[0;32m   3776\u001b[0m \u001b[39mif\u001b[39;00m core\u001b[39m.\u001b[39msymbolic_equal_dim(x_shape[x_axis], \u001b[39m0\u001b[39m):\n\u001b[0;32m   3777\u001b[0m   \u001b[39m# XLA gives error when indexing into an axis of size 0\u001b[39;00m\n\u001b[0;32m   3778\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mIndexError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mindex is out of bounds for axis \u001b[39m\u001b[39m{\u001b[39;00mx_axis\u001b[39m}\u001b[39;00m\u001b[39m with size 0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 3779\u001b[0m i \u001b[39m=\u001b[39m _normalize_index(i, x_shape[x_axis]) \u001b[39mif\u001b[39;00m normalize_indices \u001b[39melse\u001b[39;00m i\n\u001b[0;32m   3780\u001b[0m i \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39mconvert_element_type(i, index_dtype)\n\u001b[0;32m   3781\u001b[0m gather_indices\u001b[39m.\u001b[39mappend((i, \u001b[39mlen\u001b[39m(gather_indices_shape)))\n",
      "File \u001b[1;32mc:\\Users\\52333\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:3454\u001b[0m, in \u001b[0;36m_normalize_index\u001b[1;34m(index, axis_size)\u001b[0m\n\u001b[0;32m   3449\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   3450\u001b[0m   axis_size_val \u001b[39m=\u001b[39m lax\u001b[39m.\u001b[39mconvert_element_type(core\u001b[39m.\u001b[39mdimension_as_value(axis_size),\n\u001b[0;32m   3451\u001b[0m                                            _dtype(index))\n\u001b[0;32m   3452\u001b[0m \u001b[39mreturn\u001b[39;00m lax\u001b[39m.\u001b[39mselect(\n\u001b[0;32m   3453\u001b[0m   lax\u001b[39m.\u001b[39mlt(index, _lax_const(index, \u001b[39m0\u001b[39m)),\n\u001b[1;32m-> 3454\u001b[0m   lax\u001b[39m.\u001b[39;49madd(index, axis_size_val),\n\u001b[0;32m   3455\u001b[0m   index)\n",
      "File \u001b[1;32mc:\\Users\\52333\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\_src\\lax\\lax.py:402\u001b[0m, in \u001b[0;36madd\u001b[1;34m(x, y)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39madd\u001b[39m(x: Array, y: Array) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Array:\n\u001b[0;32m    401\u001b[0m   \u001b[39mr\u001b[39m\u001b[39m\"\"\"Elementwise addition: :math:`x + y`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m   \u001b[39mreturn\u001b[39;00m add_p\u001b[39m.\u001b[39;49mbind(x, y)\n",
      "File \u001b[1;32mc:\\Users\\52333\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\core.py:327\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[1;34m(self, *args, **params)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams):\n\u001b[0;32m    325\u001b[0m   \u001b[39massert\u001b[39;00m (\u001b[39mnot\u001b[39;00m config\u001b[39m.\u001b[39mjax_enable_checks \u001b[39mor\u001b[39;00m\n\u001b[0;32m    326\u001b[0m           \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(arg, Tracer) \u001b[39mor\u001b[39;00m valid_jaxtype(arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args)), args\n\u001b[1;32m--> 327\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbind_with_trace(find_top_trace(args), args, params)\n",
      "File \u001b[1;32mc:\\Users\\52333\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\core.py:330\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[1;34m(self, trace, args, params)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind_with_trace\u001b[39m(\u001b[39mself\u001b[39m, trace, args, params):\n\u001b[1;32m--> 330\u001b[0m   out \u001b[39m=\u001b[39m trace\u001b[39m.\u001b[39;49mprocess_primitive(\u001b[39mself\u001b[39;49m, \u001b[39mmap\u001b[39;49m(trace\u001b[39m.\u001b[39;49mfull_raise, args), params)\n\u001b[0;32m    331\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mmap\u001b[39m(full_lower, out) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultiple_results \u001b[39melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[1;32mc:\\Users\\52333\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\core.py:680\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[1;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_primitive\u001b[39m(\u001b[39mself\u001b[39m, primitive, tracers, params):\n\u001b[1;32m--> 680\u001b[0m   \u001b[39mreturn\u001b[39;00m primitive\u001b[39m.\u001b[39mimpl(\u001b[39m*\u001b[39mtracers, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\52333\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\_src\\dispatch.py:101\u001b[0m, in \u001b[0;36mapply_primitive\u001b[1;34m(prim, *args, **params)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[39m\"\"\"Impl rule that compiles and runs a single primitive 'prim' using XLA.\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m compiled_fun \u001b[39m=\u001b[39m xla_primitive_callable(prim, \u001b[39m*\u001b[39munsafe_map(arg_spec, args),\n\u001b[0;32m    100\u001b[0m                                       \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[1;32m--> 101\u001b[0m \u001b[39mreturn\u001b[39;00m compiled_fun(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[1;32mc:\\Users\\52333\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\_src\\dispatch.py:167\u001b[0m, in \u001b[0;36mxla_primitive_callable.<locals>.<lambda>\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    164\u001b[0m compiled \u001b[39m=\u001b[39m _xla_callable_uncached(lu\u001b[39m.\u001b[39mwrap_init(prim_fun), device, \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    165\u001b[0m                                   prim\u001b[39m.\u001b[39mname, donated_invars, \u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39marg_specs)\n\u001b[0;32m    166\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m prim\u001b[39m.\u001b[39mmultiple_results:\n\u001b[1;32m--> 167\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mlambda\u001b[39;00m \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: compiled(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    168\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m   \u001b[39mreturn\u001b[39;00m compiled\n",
      "File \u001b[1;32mc:\\Users\\52333\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\_src\\dispatch.py:712\u001b[0m, in \u001b[0;36m_execute_compiled\u001b[1;34m(name, compiled, input_handler, output_buffer_counts, result_handler, has_unordered_effects, ordered_effects, kept_var_idx, *args)\u001b[0m\n\u001b[0;32m    710\u001b[0m device, \u001b[39m=\u001b[39m compiled\u001b[39m.\u001b[39mlocal_devices()\n\u001b[0;32m    711\u001b[0m args, env \u001b[39m=\u001b[39m input_handler(args) \u001b[39mif\u001b[39;00m input_handler \u001b[39melse\u001b[39;00m (args, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 712\u001b[0m in_flat \u001b[39m=\u001b[39m flatten(device_put(x, device) \u001b[39mfor\u001b[39;49;00m i, x \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(args)\n\u001b[0;32m    713\u001b[0m                   \u001b[39mif\u001b[39;49;00m i \u001b[39min\u001b[39;49;00m kept_var_idx)\n\u001b[0;32m    714\u001b[0m \u001b[39mif\u001b[39;00m has_unordered_effects \u001b[39mor\u001b[39;00m ordered_effects:\n\u001b[0;32m    715\u001b[0m   in_flat, token_handler \u001b[39m=\u001b[39m _add_tokens(has_unordered_effects, ordered_effects,\n\u001b[0;32m    716\u001b[0m                                        device, in_flat)\n",
      "File \u001b[1;32mc:\\Users\\52333\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\_src\\util.py:111\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(xs)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcatenate\u001b[39m(xs: Iterable[Sequence[T]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[T]:\n\u001b[0;32m    110\u001b[0m   \u001b[39m\"\"\"Concatenates/flattens a list of lists.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39;49m(it\u001b[39m.\u001b[39;49mchain\u001b[39m.\u001b[39;49mfrom_iterable(xs))\n",
      "File \u001b[1;32mc:\\Users\\52333\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\_src\\dispatch.py:712\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    710\u001b[0m device, \u001b[39m=\u001b[39m compiled\u001b[39m.\u001b[39mlocal_devices()\n\u001b[0;32m    711\u001b[0m args, env \u001b[39m=\u001b[39m input_handler(args) \u001b[39mif\u001b[39;00m input_handler \u001b[39melse\u001b[39;00m (args, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m--> 712\u001b[0m in_flat \u001b[39m=\u001b[39m flatten(device_put(x, device) \u001b[39mfor\u001b[39;00m i, x \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(args)\n\u001b[0;32m    713\u001b[0m                   \u001b[39mif\u001b[39;00m i \u001b[39min\u001b[39;00m kept_var_idx)\n\u001b[0;32m    714\u001b[0m \u001b[39mif\u001b[39;00m has_unordered_effects \u001b[39mor\u001b[39;00m ordered_effects:\n\u001b[0;32m    715\u001b[0m   in_flat, token_handler \u001b[39m=\u001b[39m _add_tokens(has_unordered_effects, ordered_effects,\n\u001b[0;32m    716\u001b[0m                                        device, in_flat)\n",
      "File \u001b[1;32mc:\\Users\\52333\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\_src\\dispatch.py:978\u001b[0m, in \u001b[0;36mdevice_put\u001b[1;34m(x, device)\u001b[0m\n\u001b[0;32m    976\u001b[0m x \u001b[39m=\u001b[39m xla\u001b[39m.\u001b[39mcanonicalize_dtype(x)\n\u001b[0;32m    977\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 978\u001b[0m   \u001b[39mreturn\u001b[39;00m device_put_handlers[\u001b[39mtype\u001b[39;49m(x)](x, device)\n\u001b[0;32m    979\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m    980\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo device_put handler for type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(x)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\52333\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\jax\\_src\\dispatch.py:989\u001b[0m, in \u001b[0;36m_device_put_array\u001b[1;34m(x, device)\u001b[0m\n\u001b[0;32m    987\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m dtypes\u001b[39m.\u001b[39mfloat0:\n\u001b[0;32m    988\u001b[0m   x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(x\u001b[39m.\u001b[39mshape, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mdtype(\u001b[39mbool\u001b[39m))\n\u001b[1;32m--> 989\u001b[0m \u001b[39mreturn\u001b[39;00m (backend\u001b[39m.\u001b[39;49mbuffer_from_pyval(x, device),)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_name='lineal'\n",
    "y_hatl,y_hat_vall=modelo.model(theta, x_trainl, y_trainl, lr,n_steps,x_vl,y_vl,run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "holaa\n"
     ]
    }
   ],
   "source": [
    "#Ridge\n",
    "l=0\n",
    "importlib.reload(linear)\n",
    "\n",
    "incremento=0.1\n",
    "run_name='Ridge'\n",
    "samples=500\n",
    "x_train_r=x_train[:500]\n",
    "y_train_r=y_train[:500]\n",
    "x_vr=x_v[:500]\n",
    "y_vr=y_v[:500]\n",
    "y_hatR,y_hatR_v,precision_list=modelo.model_ridge(x_train_r,y_train_r,l,incremento,samples,k_classes,labels,x_vr,y_vr,run_name)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic\n",
    "================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1\n",
      "lambda 0.1\n",
      "-7713.833984375\n",
      "-5637.595703125\n",
      "-4684.83740234375\n",
      "-4115.771484375\n",
      "0.8593429327011108\n"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixture of Gaussians\n",
    "==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4450, 79) (5290, 79)\n",
      "79\n",
      "1\n",
      "esta es la gausiana [4.40639283e-35 3.69338316e-35 8.83044083e-35 9.61194478e-35\n",
      " 3.51080668e-35 4.74796790e-35 4.91426392e-35 1.61788793e-35\n",
      " 7.71680364e-35 5.13401204e-35] [7.04349864e-36 3.58017167e-35 1.88419400e-35 5.21552437e-36\n",
      " 1.42494976e-35 1.98734569e-35 9.97230807e-36 3.18129622e-35\n",
      " 1.07165102e-35 3.82274357e-36]\n",
      "este es el denominador [2.15337267e-35 1.41662407e-34 2.55097070e-34 5.03642953e-35\n",
      " 1.45192197e-34 1.98893127e-34 2.15924746e-34 8.89227726e-36\n",
      " 2.24155787e-34 2.38506630e-34]\n",
      "estos son los nk 992.1646012924134 2248.314808503209\n",
      "[0.25855738 0.21371023 0.20622078 0.88150117 0.75262457 0.61537186\n",
      " 0.89845326 0.51959195 0.62186534 0.15022022 0.15331979 0.18096505\n",
      " 0.54473886 0.7037893  0.62974335 0.38899897 0.58202198 0.38594292\n",
      " 0.5821853  0.00124194 0.00107264 0.00831973 0.11107336 0.01123976\n",
      " 0.0217192  0.13837279 0.41056875 0.46528621 0.6525795  0.68748489\n",
      " 0.61394998 0.71338793 0.76970508 0.76954701 0.62426307 0.38706444\n",
      " 0.74508844 0.00786604 0.52543694 0.00120666 0.00107219 0.00939595\n",
      " 0.12162311 0.01028087 0.52290457 0.14512503 0.1700655  0.16838841\n",
      " 0.10174139 0.11797058 0.12188496 0.12942171 0.21201395 0.58194008\n",
      " 0.44938105 0.59482947 0.62748253 0.75119151 0.43055347 0.57879881\n",
      " 0.70145837 0.73148573 0.74040709 0.50599612 0.74955545 0.74949256\n",
      " 0.5282524  0.2990335  0.34618021 0.34855683 0.35670159 0.36750266\n",
      " 0.36844915 0.28334644 0.0122287  0.28514778 0.59107599 0.6954983\n",
      " 0.61118813] [[ 0.00899536  0.00686458  0.00777286 ... -0.00132852 -0.00196348\n",
      "  -0.00106963]\n",
      " [ 0.00686458  0.01011984  0.00110079 ... -0.0014039  -0.00144855\n",
      "  -0.0006602 ]\n",
      " [ 0.00777286  0.00110079  0.01162851 ... -0.00076495 -0.00182961\n",
      "  -0.0011594 ]\n",
      " ...\n",
      " [-0.00132852 -0.0014039  -0.00076495 ...  0.15850239  0.14840769\n",
      "   0.13963669]\n",
      " [-0.00196348 -0.00144855 -0.00182961 ...  0.14840769  0.18155309\n",
      "   0.15355702]\n",
      " [-0.00106963 -0.0006602  -0.0011594  ...  0.13963669  0.15355702\n",
      "   0.16405257]] [2.46048325e-01 1.96865941e-01 2.13214601e-01 8.32130442e-01\n",
      " 6.90153431e-01 5.94124720e-01 8.74443455e-01 5.12882897e-01\n",
      " 5.79880280e-01 1.45029248e-01 1.49339957e-01 1.77680400e-01\n",
      " 5.43523102e-01 7.54060455e-01 6.05731320e-01 3.55312757e-01\n",
      " 5.00000000e-01 5.65700945e-01 7.03634098e-01 2.82076165e-04\n",
      " 5.54839000e-04 1.02386154e-02 1.41389322e-01 1.31233132e-02\n",
      " 0.00000000e+00 2.23171312e-01 5.31404154e-01 6.42571838e-01\n",
      " 8.17655548e-01 8.56795471e-01 7.60943618e-01 9.05166428e-01\n",
      " 9.98271968e-01 9.98669885e-01 5.00000000e-01 5.68701628e-01\n",
      " 9.88044682e-01 1.36643536e-02 7.29200573e-01 5.95892043e-04\n",
      " 8.03121510e-04 1.14615089e-02 1.74799100e-01 1.28218188e-02\n",
      " 7.66923154e-01 1.98809826e-01 2.23916513e-01 2.45975707e-01\n",
      " 1.39581938e-01 1.69733574e-01 1.60918682e-01 1.77104033e-01\n",
      " 2.96790975e-01 7.50524586e-01 6.12539781e-01 7.57552848e-01\n",
      " 8.31809549e-01 9.98921781e-01 5.81377407e-01 7.24489428e-01\n",
      " 9.07891754e-01 9.71538265e-01 9.85311068e-01 6.84873865e-01\n",
      " 9.98891400e-01 9.98685324e-01 7.31810858e-01 3.36414246e-01\n",
      " 3.85063036e-01 3.90566743e-01 4.02027992e-01 4.16402361e-01\n",
      " 4.16205546e-01 3.24831952e-01 1.31969968e-02 3.25839217e-01\n",
      " 8.14797758e-01 9.45129704e-01 8.53090704e-01] [[ 7.66525778e-03  6.00604783e-03  6.76938181e-03 ...  9.97789832e-04\n",
      "   5.42748370e-04  5.33874473e-04]\n",
      " [ 6.00604783e-03  1.03000120e-02 -2.89932238e-04 ...  2.41677440e-03\n",
      "   9.61003151e-04  1.28160616e-03]\n",
      " [ 6.76938181e-03 -2.89932238e-04  1.15722353e-02 ... -7.53791345e-04\n",
      "  -5.64225338e-05 -3.91815374e-04]\n",
      " ...\n",
      " [ 9.97789832e-04  2.41677440e-03 -7.53791345e-04 ...  5.83012506e-02\n",
      "   1.25266656e-02  2.35290390e-02]\n",
      " [ 5.42748370e-04  9.61003151e-04 -5.64225338e-05 ...  1.25266656e-02\n",
      "   2.44243985e-02  1.25726166e-02]\n",
      " [ 5.33874473e-04  1.28160616e-03 -3.91815374e-04 ...  2.35290390e-02\n",
      "   1.25726166e-02  5.18723068e-02]] [0.10186495 0.23083314]\n",
      "2\n",
      "esta es la gausiana [2.45432620e+20 7.42212153e+27 1.17610909e+25 9.02510827e+21\n",
      " 1.65178839e+28 3.48841044e+28 2.42424896e+28 2.45471862e+23\n",
      " 5.67884162e+23 2.73497363e+28] [1.71977648e+18 1.69550696e+17 1.25602207e+21 1.87499529e+25\n",
      " 2.26950725e+24 4.20460269e+20 3.69256990e+21 5.54251099e+23\n",
      " 6.91454091e+23 6.77565716e+24]\n",
      "este es el denominador [1.03622417e+21 7.56054030e+26 1.19804292e+24 7.08241792e+22\n",
      " 1.68259340e+27 3.55346751e+27 2.46945996e+27 8.44994075e+23\n",
      " 5.78474911e+22 2.78597949e+27]\n",
      "estos son los nk 3350.602350156428 5100.339486630755\n",
      "[2.67533836e-01 2.25031792e-01 2.09525787e-01 9.25646875e-01\n",
      " 8.21904330e-01 6.23244626e-01 9.13901020e-01 5.25498303e-01\n",
      " 6.67834249e-01 1.59976651e-01 1.64603726e-01 1.90195033e-01\n",
      " 5.18885706e-01 6.82708484e-01 7.24339079e-01 4.38397599e-01\n",
      " 8.43221869e-01 2.37919247e-02 2.26348022e-01 7.20760348e-04\n",
      " 5.62488423e-04 2.31348936e-03 4.09427368e-02 4.98265634e-03\n",
      " 4.07488161e-02 8.82140327e-03 1.01303779e-01 1.18519627e-01\n",
      " 1.62824736e-01 1.70205249e-01 1.53860241e-01 1.78351682e-01\n",
      " 1.91469930e-01 1.91045051e-01 9.58425035e-01 2.38136065e-02\n",
      " 8.21504757e-02 1.28107208e-03 5.69498016e-02 3.54842469e-04\n",
      " 5.03690931e-04 5.10277870e-04 8.67306083e-03 1.46206569e-03\n",
      " 3.23323224e-02 1.53558740e-02 2.01033291e-02 1.90728151e-02\n",
      " 1.20406083e-02 1.19080430e-02 1.40182503e-02 1.35746580e-02\n",
      " 2.06241270e-02 6.75066341e-02 4.91165061e-02 6.80804234e-02\n",
      " 6.86443887e-02 8.31364245e-02 4.60464321e-02 6.61506771e-02\n",
      " 7.82113914e-02 8.19922142e-02 8.19533037e-02 5.36825485e-02\n",
      " 8.28687212e-02 8.30587047e-02 5.81913217e-02 3.61047120e-02\n",
      " 4.01972454e-02 3.98263337e-02 4.06955320e-02 4.18910675e-02\n",
      " 4.18303244e-02 3.07061325e-02 1.06100688e-03 3.22292316e-02\n",
      " 6.64695134e-02 7.74365799e-02 6.81006765e-02] [[ 1.12837296e-02  8.60509640e-03  9.75105836e-03 ... -3.77537800e-04\n",
      "  -4.57031460e-04 -2.52708651e-04]\n",
      " [ 8.60509640e-03  1.13565874e-02  2.70539546e-03 ... -5.81987918e-04\n",
      "  -7.01285178e-04 -5.46356252e-04]\n",
      " [ 9.75105836e-03  2.70539546e-03  1.32378429e-02 ... -1.41277446e-04\n",
      "  -1.75810266e-04  1.46549772e-05]\n",
      " ...\n",
      " [-3.77537800e-04 -5.81987918e-04 -1.41277446e-04 ...  5.33274738e-02\n",
      "   5.81430362e-02  5.21101812e-02]\n",
      " [-4.57031460e-04 -7.01285178e-04 -1.75810266e-04 ...  5.81430362e-02\n",
      "   6.82891988e-02  5.96867428e-02]\n",
      " [-2.52708651e-04 -5.46356252e-04  1.46549772e-05 ...  5.21101812e-02\n",
      "   5.96867428e-02  5.54946863e-02]] [2.48423185e-01 1.98132719e-01 2.15905922e-01 8.39104408e-01\n",
      " 7.02150507e-01 5.97845809e-01 8.79923786e-01 5.13071859e-01\n",
      " 5.86562466e-01 1.45516637e-01 1.50011634e-01 1.78608664e-01\n",
      " 5.49259539e-01 7.57184980e-01 5.93389403e-01 3.48751580e-01\n",
      " 5.00000000e-01 5.65150342e-01 6.78847499e-01 2.04293101e-04\n",
      " 3.51560429e-04 9.44765563e-03 1.29473800e-01 1.36678596e-02\n",
      " 0.00000000e+00 2.26667500e-01 5.33891372e-01 6.54788818e-01\n",
      " 8.23423913e-01 8.65910149e-01 7.65318186e-01 9.09397720e-01\n",
      " 9.98430253e-01 9.98318412e-01 5.00000000e-01 5.67967978e-01\n",
      " 9.88164767e-01 1.36385680e-02 7.02282383e-01 4.75311671e-04\n",
      " 5.79264558e-04 1.04287449e-02 1.62562624e-01 1.33187353e-02\n",
      " 7.63729201e-01 2.07936522e-01 2.35653046e-01 2.57390129e-01\n",
      " 1.42242838e-01 1.75340059e-01 1.71541953e-01 1.88139395e-01\n",
      " 3.07952855e-01 7.59809966e-01 6.19258728e-01 7.67876588e-01\n",
      " 8.30841970e-01 9.98657938e-01 5.85563531e-01 7.39218977e-01\n",
      " 9.10210043e-01 9.73016624e-01 9.85656546e-01 6.89569582e-01\n",
      " 9.99063303e-01 9.98402011e-01 7.04665275e-01 3.97398920e-01\n",
      " 4.53311819e-01 4.57885727e-01 4.71726898e-01 4.88857378e-01\n",
      " 4.88695943e-01 3.80472348e-01 1.55329296e-02 3.82459459e-01\n",
      " 8.20746007e-01 9.46480158e-01 8.58787606e-01] [[ 7.70254245e-03  5.97205778e-03  6.86551297e-03 ...  9.55703578e-04\n",
      "   5.66921114e-04  4.92304148e-04]\n",
      " [ 5.97205778e-03  1.02832495e-02 -3.29819867e-04 ...  2.41164348e-03\n",
      "   9.82023976e-04  1.21936691e-03]\n",
      " [ 6.86551297e-03 -3.29819867e-04  1.17723415e-02 ... -8.18804179e-04\n",
      "  -3.71554529e-05 -3.98859993e-04]\n",
      " ...\n",
      " [ 9.55703578e-04  2.41164348e-03 -8.18804179e-04 ...  5.74950025e-02\n",
      "   1.19914484e-02  2.26447502e-02]\n",
      " [ 5.66921114e-04  9.82023976e-04 -3.71554529e-05 ...  1.19914484e-02\n",
      "   2.38955476e-02  1.19809409e-02]\n",
      " [ 4.92304148e-04  1.21936691e-03 -3.98859993e-04 ...  2.26447502e-02\n",
      "   1.19809409e-02  5.06652567e-02]] [0.34400435 0.52364882]\n",
      "3\n",
      "esta es la gausiana [4.78577363e+08 1.50009245e+36 1.79905275e+32 3.91682045e+08\n",
      " 2.37576426e+36 4.55411249e+36 3.12103578e+36 1.93157423e+15\n",
      " 1.04985846e+31 4.64104769e+36] [5.28169374e+17 7.91803687e+16 1.18470639e+21 1.81868478e+25\n",
      " 9.90065394e+23 1.61287588e+20 4.15640600e+21 2.44434964e+23\n",
      " 3.20208963e+23 6.02398159e+24]\n",
      "este es el denominador [1.64034616e+21 5.16038326e+35 6.18881969e+31 1.28615621e+23\n",
      " 8.17273234e+35 1.56663450e+36 1.07364988e+36 1.03666140e+24\n",
      " 3.61155874e+30 1.59654058e+36]\n",
      "estos son los nk 3199.3809575366795 5286.822215039805\n",
      "[2.66927618e-01 2.24913906e-01 2.08769975e-01 9.25715220e-01\n",
      " 8.23837106e-01 6.21861413e-01 9.14086304e-01 5.25021605e-01\n",
      " 6.68832975e-01 1.60420715e-01 1.65352626e-01 1.90753922e-01\n",
      " 5.16953781e-01 6.81326058e-01 7.31323330e-01 4.42437190e-01\n",
      " 8.59444535e-01 6.85975046e-04 2.05983877e-01 4.19975803e-04\n",
      " 6.78816918e-05 2.09719997e-03 3.85996026e-02 4.59161039e-03\n",
      " 4.26745136e-02 2.84121419e-04 8.08626136e-02 9.77719197e-02\n",
      " 1.29670826e-01 1.34548900e-01 1.22525470e-01 1.42193537e-01\n",
      " 1.53274852e-01 1.53022014e-01 9.80092874e-01 6.85975414e-04\n",
      " 3.88495432e-02 1.26152666e-03 2.82740976e-02 1.93574949e-12\n",
      " 2.74793283e-08 6.64351023e-05 3.04506028e-03 8.96439711e-04\n",
      " 6.37469874e-04 8.43438066e-03 1.19881232e-02 1.09115838e-02\n",
      " 6.88268110e-03 6.12271612e-03 7.81060124e-03 7.24417034e-03\n",
      " 1.02579119e-02 3.30079040e-02 2.38195936e-02 3.29303591e-02\n",
      " 3.31623774e-02 3.98142513e-02 2.18399037e-02 3.02405050e-02\n",
      " 3.62344131e-02 3.95573493e-02 3.91656576e-02 2.54802262e-02\n",
      " 3.98142513e-02 3.98142513e-02 2.96064345e-02 1.42589195e-02\n",
      " 1.37872188e-02 1.35364900e-02 1.41090556e-02 1.48071142e-02\n",
      " 1.46210942e-02 1.04486466e-02 2.43347598e-04 1.11929947e-02\n",
      " 3.17176096e-02 3.74230998e-02 3.31975661e-02] [[ 0.01128386  0.00863335  0.00972527 ... -0.00084586 -0.00103586\n",
      "  -0.00088105]\n",
      " [ 0.00863335  0.01136884  0.00274542 ... -0.00068532 -0.00093975\n",
      "  -0.00083724]\n",
      " [ 0.00972527  0.00274542  0.01315268 ... -0.00071775 -0.00077966\n",
      "  -0.000634  ]\n",
      " ...\n",
      " [-0.00084586 -0.00068532 -0.00071775 ...  0.02647344  0.02917786\n",
      "   0.02621127]\n",
      " [-0.00103586 -0.00093975 -0.00077966 ...  0.02917786  0.03467688\n",
      "   0.03063455]\n",
      " [-0.00088105 -0.00083724 -0.000634   ...  0.02621127  0.03063455\n",
      "   0.02852928]] [2.48970599e-01 1.98813924e-01 2.16137074e-01 8.42345079e-01\n",
      " 7.05601731e-01 5.99953943e-01 8.81974383e-01 5.13885176e-01\n",
      " 5.89244778e-01 1.45598238e-01 1.49989170e-01 1.78598493e-01\n",
      " 5.49606919e-01 7.55827715e-01 5.92460156e-01 3.49094557e-01\n",
      " 5.00000000e-01 5.63270522e-01 6.77023778e-01 2.51863579e-04\n",
      " 5.33057754e-04 9.25256507e-03 1.27801912e-01 1.37110983e-02\n",
      " 0.00000000e+00 2.26133987e-01 5.33572282e-01 6.52263611e-01\n",
      " 8.25542435e-01 8.68257435e-01 7.67393476e-01 9.11005161e-01\n",
      " 9.98297654e-01 9.98297654e-01 5.00000000e-01 5.66013190e-01\n",
      " 9.88556453e-01 1.31716863e-02 7.00160439e-01 5.46006471e-04\n",
      " 7.56598122e-04 1.02392952e-02 1.60971711e-01 1.33715716e-02\n",
      " 7.62169838e-01 2.06157311e-01 2.34041061e-01 2.55015853e-01\n",
      " 1.41522133e-01 1.74109640e-01 1.70458110e-01 1.86782792e-01\n",
      " 3.04985655e-01 7.61389064e-01 6.17556186e-01 7.69994737e-01\n",
      " 8.30960529e-01 9.98675954e-01 5.84264802e-01 7.42474037e-01\n",
      " 9.12518342e-01 9.73708214e-01 9.85908360e-01 6.88663242e-01\n",
      " 9.98959678e-01 9.98392229e-01 7.03050979e-01 4.01818273e-01\n",
      " 4.59318156e-01 4.63857797e-01 4.77476509e-01 4.94216234e-01\n",
      " 4.94029106e-01 3.83292189e-01 1.55464084e-02 3.86439000e-01\n",
      " 8.21067875e-01 9.46187017e-01 8.58139533e-01] [[ 7.95429216e-03  6.17683328e-03  7.08032033e-03 ...  9.74167982e-04\n",
      "   5.64089834e-04  5.01659307e-04]\n",
      " [ 6.17683328e-03  1.04563043e-02 -1.61582194e-04 ...  2.42635485e-03\n",
      "   9.97001030e-04  1.22661799e-03]\n",
      " [ 7.08032033e-03 -1.61582194e-04  1.19621161e-02 ... -8.02741549e-04\n",
      "  -5.68513072e-05 -3.90519146e-04]\n",
      " ...\n",
      " [ 9.74167982e-04  2.42635485e-03 -8.02741549e-04 ...  5.74493572e-02\n",
      "   1.21705941e-02  2.28491159e-02]\n",
      " [ 5.64089834e-04  9.97001030e-04 -5.68513072e-05 ...  1.21705941e-02\n",
      "   2.40106545e-02  1.21794517e-02]\n",
      " [ 5.01659307e-04  1.22661799e-03 -3.90519146e-04 ...  2.28491159e-02\n",
      "   1.21794517e-02  5.08058415e-02]] [0.32847854 0.54279489]\n",
      "4\n",
      "esta es la gausiana [6.58848971e-28 2.21798705e+37 1.88660493e+33 6.68394520e-27\n",
      " 3.44197032e+37 6.66174293e+37 4.51438000e+37 1.41595987e-17\n",
      " 1.08192274e+32 6.74420617e+37] [5.33718433e+17 6.35833679e+16 1.20830710e+21 2.20094361e+25\n",
      " 1.07319817e+24 1.57546056e+20 3.99492649e+21 2.57289060e+23\n",
      " 4.34122784e+23 7.89718800e+24]\n",
      "este es el denominador [1.95195308e+21 7.28561144e+36 6.19709228e+32 1.44385967e+23\n",
      " 1.13061338e+37 2.18823958e+37 1.48287694e+37 1.25057245e+24\n",
      " 3.55388399e+31 2.21532698e+37]\n",
      "estos son los nk 3194.7232552738137 5290.0\n",
      "[2.66661884e-01 2.24766785e-01 2.08495753e-01 9.25651114e-01\n",
      " 8.23651159e-01 6.21995722e-01 9.14039121e-01 5.25049599e-01\n",
      " 6.68712203e-01 1.60414653e-01 1.65377860e-01 1.90699812e-01\n",
      " 5.16880235e-01 6.81311536e-01 7.31651309e-01 4.42740000e-01\n",
      " 8.59968582e-01 1.62607889e-23 2.05533331e-01 4.20588090e-04\n",
      " 6.79531397e-05 2.10025160e-03 3.86506612e-02 4.56915129e-03\n",
      " 4.27123097e-02 5.33376266e-24 8.02733955e-02 9.72250858e-02\n",
      " 1.28501922e-01 1.33386248e-01 1.21560115e-01 1.41030746e-01\n",
      " 1.52125853e-01 1.51872695e-01 9.80792819e-01 1.62607889e-23\n",
      " 3.74482477e-02 1.26336543e-03 2.74872468e-02 1.67050397e-41\n",
      " 4.41972893e-40 6.65218493e-05 3.01912537e-03 8.79188528e-04\n",
      " 1.11892913e-23 8.18599201e-03 1.16605518e-02 1.05417959e-02\n",
      " 6.65471746e-03 5.89109178e-03 7.50138225e-03 6.99419062e-03\n",
      " 9.88240949e-03 3.16488678e-02 2.30714264e-02 3.15720386e-02\n",
      " 3.20579187e-02 3.84143619e-02 2.11345775e-02 2.88347017e-02\n",
      " 3.48721568e-02 3.81624152e-02 3.78269892e-02 2.46757754e-02\n",
      " 3.84143619e-02 3.84143619e-02 2.89189525e-02 1.29210939e-02\n",
      " 1.24926998e-02 1.21912886e-02 1.27626052e-02 1.34596948e-02\n",
      " 1.32734881e-02 9.70891591e-03 2.35024160e-04 1.01551272e-02\n",
      " 3.03444761e-02 3.60228068e-02 3.18275883e-02] [[ 0.01114381  0.00857165  0.00956132 ... -0.00110507 -0.00129357\n",
      "  -0.00114039]\n",
      " [ 0.00857165  0.01134031  0.00267475 ... -0.00082792 -0.00108297\n",
      "  -0.00098064]\n",
      " [ 0.00956132  0.00267475  0.01295851 ... -0.00098668 -0.00104521\n",
      "  -0.00090234]\n",
      " ...\n",
      " [-0.00110507 -0.00082792 -0.00098668 ...  0.02520734  0.02789914\n",
      "   0.02494966]\n",
      " [-0.00129357 -0.00108297 -0.00104521 ...  0.02789914  0.03337995\n",
      "   0.02936004]\n",
      " [-0.00114039 -0.00098064 -0.00090234 ...  0.02494966  0.02936004\n",
      "   0.0272718 ]] [2.49045369e-01 1.98850032e-01 2.16225583e-01 8.42438563e-01\n",
      " 7.05765595e-01 6.00094518e-01 8.82041588e-01 5.13988658e-01\n",
      " 5.89413989e-01 1.45578660e-01 1.49968494e-01 1.78575929e-01\n",
      " 5.49657372e-01 7.55765595e-01 5.92405971e-01 3.49073724e-01\n",
      " 5.00000000e-01 5.63232514e-01 6.76916614e-01 2.52047889e-04\n",
      " 5.32737584e-04 9.24700693e-03 1.27731569e-01 1.37128197e-02\n",
      " 0.00000000e+00 2.26107961e-01 5.33553875e-01 6.52173913e-01\n",
      " 8.25614367e-01 8.68336484e-01 7.67485822e-01 9.11058601e-01\n",
      " 9.98298677e-01 9.98298677e-01 5.00000000e-01 5.65973535e-01\n",
      " 9.88563327e-01 1.31637738e-02 7.00047259e-01 5.46103760e-04\n",
      " 7.56143667e-04 1.02331443e-02 1.60888469e-01 1.33733266e-02\n",
      " 7.62066793e-01 2.06112161e-01 2.34026465e-01 2.54988448e-01\n",
      " 1.41503886e-01 1.74081075e-01 1.70426381e-01 1.86767486e-01\n",
      " 3.04935938e-01 7.61531191e-01 6.17485822e-01 7.70132325e-01\n",
      " 8.30907372e-01 9.98676749e-01 5.84215501e-01 7.42627599e-01\n",
      " 9.12570888e-01 9.73724008e-01 9.85916824e-01 6.88563327e-01\n",
      " 9.98960302e-01 9.98393195e-01 7.02930057e-01 4.02173913e-01\n",
      " 4.59640832e-01 4.64177694e-01 4.77788280e-01 4.94517958e-01\n",
      " 4.94328922e-01 3.83364839e-01 1.55372248e-02 3.86683470e-01\n",
      " 8.21172023e-01 9.46219282e-01 8.58223062e-01] [[ 7.97309624e-03  6.18462999e-03  7.10386374e-03 ...  9.87130192e-04\n",
      "   5.67751866e-04  5.12103302e-04]\n",
      " [ 6.18462999e-03  1.04573522e-02 -1.49635567e-04 ...  2.43140450e-03\n",
      "   9.98330302e-04  1.23110088e-03]\n",
      " [ 7.10386374e-03 -1.49635567e-04  1.19894085e-02 ... -7.86187514e-04\n",
      "  -5.20771915e-05 -3.77595373e-04]\n",
      " ...\n",
      " [ 9.87130192e-04  2.43140450e-03 -7.86187514e-04 ...  5.74345432e-02\n",
      "   1.21688923e-02  2.28504758e-02]\n",
      " [ 5.67751866e-04  9.98330302e-04 -5.20771915e-05 ...  1.21688923e-02\n",
      "   2.39979935e-02  1.21766467e-02]\n",
      " [ 5.12103302e-04  1.23110088e-03 -3.77595373e-04 ...  2.28504758e-02\n",
      "   1.21766467e-02  5.07877688e-02]] [0.32800033 0.54312115]\n",
      "5\n",
      "esta es la gausiana [9.81809052e-37 2.55061094e+37 2.17897602e+33 5.24712900e-35\n",
      " 3.94840753e+37 7.65668049e+37 5.18628421e+37 1.28113994e-21\n",
      " 1.24904941e+32 7.74284970e+37] [5.33865211e+17 6.31544425e+16 1.21114544e+21 2.21092892e+25\n",
      " 1.07209668e+24 1.57062973e+20 3.98419090e+21 2.59284521e+23\n",
      " 4.36193338e+23 7.98752857e+24]\n",
      "este es el denominador [1.96040564e+21 8.36601242e+36 7.14704864e+32 1.44525979e+23\n",
      " 1.29507899e+37 2.51139376e+37 1.70110295e+37 1.25294468e+24\n",
      " 4.09688625e+31 2.53965729e+37]\n",
      "estos son los nk 3194.0009523492286 5290.0\n",
      "[2.66631731e-01 2.24723388e-01 2.08486367e-01 9.25747373e-01\n",
      " 8.23611278e-01 6.22023310e-01 9.14019682e-01 5.25055264e-01\n",
      " 6.68750356e-01 1.60425802e-01 1.65365005e-01 1.90667556e-01\n",
      " 5.16778048e-01 6.81352539e-01 7.31684200e-01 4.42704437e-01\n",
      " 8.60049987e-01 3.11153386e-26 2.05429048e-01 4.20683203e-04\n",
      " 6.79685068e-05 2.10072656e-03 3.86594018e-02 4.56787699e-03\n",
      " 4.26573563e-02 7.26643911e-27 8.02915488e-02 9.72470726e-02\n",
      " 1.28530982e-01 1.33416413e-01 1.21587605e-01 1.41062639e-01\n",
      " 1.52160255e-01 1.51907040e-01 9.80901547e-01 3.11153386e-26\n",
      " 3.72305727e-02 1.26365113e-03 2.73521232e-02 6.00007574e-44\n",
      " 4.26015378e-43 6.65368928e-05 3.01980813e-03 8.77079763e-04\n",
      " 2.33448043e-26 7.96169960e-03 1.14370451e-02 1.03180363e-02\n",
      " 6.43007877e-03 5.66628040e-03 7.27693502e-03 6.76962869e-03\n",
      " 9.65850073e-03 3.15429532e-02 2.28505002e-02 3.14661066e-02\n",
      " 3.18390248e-02 3.81969054e-02 2.10262851e-02 2.86150788e-02\n",
      " 3.47669711e-02 3.79449017e-02 3.77224717e-02 2.44552120e-02\n",
      " 3.81969054e-02 3.81969054e-02 2.88124205e-02 1.29240160e-02\n",
      " 1.24955250e-02 1.21940456e-02 1.27654913e-02 1.34627386e-02\n",
      " 1.32764898e-02 9.71111151e-03 2.35077309e-04 1.01574237e-02\n",
      " 3.01251947e-02 3.58048095e-02 3.16086422e-02] [[ 0.01114231  0.0085678   0.00956223 ... -0.00113457 -0.00132293\n",
      "  -0.00116985]\n",
      " [ 0.0085678   0.01133455  0.00267355 ... -0.00087019 -0.00112505\n",
      "  -0.00102288]\n",
      " [ 0.00956223  0.00267355  0.01296105 ... -0.000996   -0.00105449\n",
      "  -0.00091164]\n",
      " ...\n",
      " [-0.00113457 -0.00087019 -0.000996   ...  0.02500037  0.02769402\n",
      "   0.02474295]\n",
      " [-0.00132293 -0.00112505 -0.00105449 ...  0.02769402  0.03317731\n",
      "   0.02915557]\n",
      " [-0.00116985 -0.00102288 -0.00091164 ...  0.02474295  0.02915557\n",
      "   0.02706594]] [2.49045369e-01 1.98850032e-01 2.16225583e-01 8.42438563e-01\n",
      " 7.05765595e-01 6.00094518e-01 8.82041588e-01 5.13988658e-01\n",
      " 5.89413989e-01 1.45578660e-01 1.49968494e-01 1.78575929e-01\n",
      " 5.49657372e-01 7.55765595e-01 5.92405971e-01 3.49073724e-01\n",
      " 5.00000000e-01 5.63232514e-01 6.76916614e-01 2.52047889e-04\n",
      " 5.32737584e-04 9.24700693e-03 1.27731569e-01 1.37128197e-02\n",
      " 0.00000000e+00 2.26107961e-01 5.33553875e-01 6.52173913e-01\n",
      " 8.25614367e-01 8.68336484e-01 7.67485822e-01 9.11058601e-01\n",
      " 9.98298677e-01 9.98298677e-01 5.00000000e-01 5.65973535e-01\n",
      " 9.88563327e-01 1.31637738e-02 7.00047259e-01 5.46103760e-04\n",
      " 7.56143667e-04 1.02331443e-02 1.60888469e-01 1.33733266e-02\n",
      " 7.62066793e-01 2.06112161e-01 2.34026465e-01 2.54988448e-01\n",
      " 1.41503886e-01 1.74081075e-01 1.70426381e-01 1.86767486e-01\n",
      " 3.04935938e-01 7.61531191e-01 6.17485822e-01 7.70132325e-01\n",
      " 8.30907372e-01 9.98676749e-01 5.84215501e-01 7.42627599e-01\n",
      " 9.12570888e-01 9.73724008e-01 9.85916824e-01 6.88563327e-01\n",
      " 9.98960302e-01 9.98393195e-01 7.02930057e-01 4.02173913e-01\n",
      " 4.59640832e-01 4.64177694e-01 4.77788280e-01 4.94517958e-01\n",
      " 4.94328922e-01 3.83364839e-01 1.55372248e-02 3.86683470e-01\n",
      " 8.21172023e-01 9.46219282e-01 8.58223062e-01] [[ 7.97309624e-03  6.18462999e-03  7.10386374e-03 ...  9.87130192e-04\n",
      "   5.67751866e-04  5.12103302e-04]\n",
      " [ 6.18462999e-03  1.04573522e-02 -1.49635567e-04 ...  2.43140450e-03\n",
      "   9.98330302e-04  1.23110088e-03]\n",
      " [ 7.10386374e-03 -1.49635567e-04  1.19894085e-02 ... -7.86187514e-04\n",
      "  -5.20771915e-05 -3.77595373e-04]\n",
      " ...\n",
      " [ 9.87130192e-04  2.43140450e-03 -7.86187514e-04 ...  5.74345432e-02\n",
      "   1.21688923e-02  2.28504758e-02]\n",
      " [ 5.67751866e-04  9.98330302e-04 -5.20771915e-05 ...  1.21688923e-02\n",
      "   2.39979935e-02  1.21766467e-02]\n",
      " [ 5.12103302e-04  1.23110088e-03 -3.77595373e-04 ...  2.28504758e-02\n",
      "   1.21766467e-02  5.07877688e-02]] [0.32792618 0.54312115]\n",
      "6\n",
      "esta es la gausiana [1.04191848e-36 2.68231455e+37 2.29394558e+33 5.49462842e-35\n",
      " 4.14847897e+37 8.04591555e+37 5.45336254e+37 1.35157897e-21\n",
      " 1.31363120e+32 8.14350180e+37] [5.33865211e+17 6.31544425e+16 1.21114544e+21 2.21092892e+25\n",
      " 1.07209668e+24 1.57062973e+20 3.98419090e+21 2.59284521e+23\n",
      " 4.36193338e+23 7.98752857e+24]\n",
      "este es el denominador [1.96040564e+21 8.79601154e+36 7.52244802e+32 1.44525979e+23\n",
      " 1.36039484e+37 2.63846632e+37 1.78830032e+37 1.25294468e+24\n",
      " 4.30774056e+31 2.67046740e+37]\n",
      "estos son los nk 3193.9997782439873 5290.0\n",
      "[2.66631699e-01 2.24723340e-01 2.08486357e-01 9.25747494e-01\n",
      " 8.23611214e-01 6.22023355e-01 9.14019650e-01 5.25055273e-01\n",
      " 6.68750384e-01 1.60425820e-01 1.65364991e-01 1.90667519e-01\n",
      " 5.16777894e-01 6.81352571e-01 7.31684256e-01 4.42704407e-01\n",
      " 8.60050119e-01 3.24758681e-26 2.05428879e-01 4.20683358e-04\n",
      " 6.79685318e-05 2.10072733e-03 3.86594091e-02 4.56787563e-03\n",
      " 4.26572670e-02 7.58835022e-27 8.02915784e-02 9.72471084e-02\n",
      " 1.28531029e-01 1.33416462e-01 1.21587650e-01 1.41062691e-01\n",
      " 1.52160311e-01 1.51907096e-01 9.80901724e-01 3.24758681e-26\n",
      " 3.72302188e-02 1.26365159e-03 2.73518948e-02 4.80032217e-44\n",
      " 3.73848765e-43 6.65369172e-05 3.01980924e-03 8.77076335e-04\n",
      " 2.43781399e-26 7.96133493e-03 1.14366818e-02 1.03176725e-02\n",
      " 6.42971354e-03 5.66591488e-03 7.27657010e-03 6.76926358e-03\n",
      " 9.65819068e-03 3.15427463e-02 2.28501757e-02 3.14659344e-02\n",
      " 3.18386689e-02 3.81965519e-02 2.10261091e-02 2.86147218e-02\n",
      " 3.47667654e-02 3.79445481e-02 3.77222671e-02 2.44548534e-02\n",
      " 3.81965519e-02 3.81965519e-02 2.88122473e-02 1.29239513e-02\n",
      " 1.24954601e-02 1.21939806e-02 1.27654266e-02 1.34626741e-02\n",
      " 1.32764253e-02 9.71108037e-03 2.35077396e-04 1.01573734e-02\n",
      " 3.01248382e-02 3.58044551e-02 3.16082863e-02] [[ 0.01114231  0.0085678   0.00956223 ... -0.0011346  -0.00132297\n",
      "  -0.00116988]\n",
      " [ 0.0085678   0.01133454  0.00267355 ... -0.00087024 -0.0011251\n",
      "  -0.00102293]\n",
      " [ 0.00956223  0.00267355  0.01296106 ... -0.00099601 -0.0010545\n",
      "  -0.00091165]\n",
      " ...\n",
      " [-0.0011346  -0.00087024 -0.00099601 ...  0.02500003  0.02769369\n",
      "   0.02474261]\n",
      " [-0.00132297 -0.0011251  -0.0010545  ...  0.02769369  0.03317698\n",
      "   0.02915524]\n",
      " [-0.00116988 -0.00102293 -0.00091165 ...  0.02474261  0.02915524\n",
      "   0.02706561]] [2.49045369e-01 1.98850032e-01 2.16225583e-01 8.42438563e-01\n",
      " 7.05765595e-01 6.00094518e-01 8.82041588e-01 5.13988658e-01\n",
      " 5.89413989e-01 1.45578660e-01 1.49968494e-01 1.78575929e-01\n",
      " 5.49657372e-01 7.55765595e-01 5.92405971e-01 3.49073724e-01\n",
      " 5.00000000e-01 5.63232514e-01 6.76916614e-01 2.52047889e-04\n",
      " 5.32737584e-04 9.24700693e-03 1.27731569e-01 1.37128197e-02\n",
      " 0.00000000e+00 2.26107961e-01 5.33553875e-01 6.52173913e-01\n",
      " 8.25614367e-01 8.68336484e-01 7.67485822e-01 9.11058601e-01\n",
      " 9.98298677e-01 9.98298677e-01 5.00000000e-01 5.65973535e-01\n",
      " 9.88563327e-01 1.31637738e-02 7.00047259e-01 5.46103760e-04\n",
      " 7.56143667e-04 1.02331443e-02 1.60888469e-01 1.33733266e-02\n",
      " 7.62066793e-01 2.06112161e-01 2.34026465e-01 2.54988448e-01\n",
      " 1.41503886e-01 1.74081075e-01 1.70426381e-01 1.86767486e-01\n",
      " 3.04935938e-01 7.61531191e-01 6.17485822e-01 7.70132325e-01\n",
      " 8.30907372e-01 9.98676749e-01 5.84215501e-01 7.42627599e-01\n",
      " 9.12570888e-01 9.73724008e-01 9.85916824e-01 6.88563327e-01\n",
      " 9.98960302e-01 9.98393195e-01 7.02930057e-01 4.02173913e-01\n",
      " 4.59640832e-01 4.64177694e-01 4.77788280e-01 4.94517958e-01\n",
      " 4.94328922e-01 3.83364839e-01 1.55372248e-02 3.86683470e-01\n",
      " 8.21172023e-01 9.46219282e-01 8.58223062e-01] [[ 7.97309624e-03  6.18462999e-03  7.10386374e-03 ...  9.87130192e-04\n",
      "   5.67751866e-04  5.12103302e-04]\n",
      " [ 6.18462999e-03  1.04573522e-02 -1.49635567e-04 ...  2.43140450e-03\n",
      "   9.98330302e-04  1.23110088e-03]\n",
      " [ 7.10386374e-03 -1.49635567e-04  1.19894085e-02 ... -7.86187514e-04\n",
      "  -5.20771915e-05 -3.77595373e-04]\n",
      " ...\n",
      " [ 9.87130192e-04  2.43140450e-03 -7.86187514e-04 ...  5.74345432e-02\n",
      "   1.21688923e-02  2.28504758e-02]\n",
      " [ 5.67751866e-04  9.98330302e-04 -5.20771915e-05 ...  1.21688923e-02\n",
      "   2.39979935e-02  1.21766467e-02]\n",
      " [ 5.12103302e-04  1.23110088e-03 -3.77595373e-04 ...  2.28504758e-02\n",
      "   1.21766467e-02  5.07877688e-02]] [0.32792606 0.54312115]\n",
      "estps son los mu1 y mu2 [2.66631699e-01 2.24723340e-01 2.08486357e-01 9.25747494e-01\n",
      " 8.23611214e-01 6.22023355e-01 9.14019650e-01 5.25055273e-01\n",
      " 6.68750384e-01 1.60425820e-01 1.65364991e-01 1.90667519e-01\n",
      " 5.16777894e-01 6.81352571e-01 7.31684256e-01 4.42704407e-01\n",
      " 8.60050119e-01 3.24758681e-26 2.05428879e-01 4.20683358e-04\n",
      " 6.79685318e-05 2.10072733e-03 3.86594091e-02 4.56787563e-03\n",
      " 4.26572670e-02 7.58835022e-27 8.02915784e-02 9.72471084e-02\n",
      " 1.28531029e-01 1.33416462e-01 1.21587650e-01 1.41062691e-01\n",
      " 1.52160311e-01 1.51907096e-01 9.80901724e-01 3.24758681e-26\n",
      " 3.72302188e-02 1.26365159e-03 2.73518948e-02 4.80032217e-44\n",
      " 3.73848765e-43 6.65369172e-05 3.01980924e-03 8.77076335e-04\n",
      " 2.43781399e-26 7.96133493e-03 1.14366818e-02 1.03176725e-02\n",
      " 6.42971354e-03 5.66591488e-03 7.27657010e-03 6.76926358e-03\n",
      " 9.65819068e-03 3.15427463e-02 2.28501757e-02 3.14659344e-02\n",
      " 3.18386689e-02 3.81965519e-02 2.10261091e-02 2.86147218e-02\n",
      " 3.47667654e-02 3.79445481e-02 3.77222671e-02 2.44548534e-02\n",
      " 3.81965519e-02 3.81965519e-02 2.88122473e-02 1.29239513e-02\n",
      " 1.24954601e-02 1.21939806e-02 1.27654266e-02 1.34626741e-02\n",
      " 1.32764253e-02 9.71108037e-03 2.35077396e-04 1.01573734e-02\n",
      " 3.01248382e-02 3.58044551e-02 3.16082863e-02] [2.49045369e-01 1.98850032e-01 2.16225583e-01 8.42438563e-01\n",
      " 7.05765595e-01 6.00094518e-01 8.82041588e-01 5.13988658e-01\n",
      " 5.89413989e-01 1.45578660e-01 1.49968494e-01 1.78575929e-01\n",
      " 5.49657372e-01 7.55765595e-01 5.92405971e-01 3.49073724e-01\n",
      " 5.00000000e-01 5.63232514e-01 6.76916614e-01 2.52047889e-04\n",
      " 5.32737584e-04 9.24700693e-03 1.27731569e-01 1.37128197e-02\n",
      " 0.00000000e+00 2.26107961e-01 5.33553875e-01 6.52173913e-01\n",
      " 8.25614367e-01 8.68336484e-01 7.67485822e-01 9.11058601e-01\n",
      " 9.98298677e-01 9.98298677e-01 5.00000000e-01 5.65973535e-01\n",
      " 9.88563327e-01 1.31637738e-02 7.00047259e-01 5.46103760e-04\n",
      " 7.56143667e-04 1.02331443e-02 1.60888469e-01 1.33733266e-02\n",
      " 7.62066793e-01 2.06112161e-01 2.34026465e-01 2.54988448e-01\n",
      " 1.41503886e-01 1.74081075e-01 1.70426381e-01 1.86767486e-01\n",
      " 3.04935938e-01 7.61531191e-01 6.17485822e-01 7.70132325e-01\n",
      " 8.30907372e-01 9.98676749e-01 5.84215501e-01 7.42627599e-01\n",
      " 9.12570888e-01 9.73724008e-01 9.85916824e-01 6.88563327e-01\n",
      " 9.98960302e-01 9.98393195e-01 7.02930057e-01 4.02173913e-01\n",
      " 4.59640832e-01 4.64177694e-01 4.77788280e-01 4.94517958e-01\n",
      " 4.94328922e-01 3.83364839e-01 1.55372248e-02 3.86683470e-01\n",
      " 8.21172023e-01 9.46219282e-01 8.58223062e-01]\n",
      "La precision, accuracy y recall 0.19187289476394653 0.1289527714252472 0.2822471857070923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.19187289476394653"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(mixture)\n",
    "\n",
    "clases=int(2)\n",
    "\n",
    "\n",
    "x11,x12=fe.separate(x_train,y_train)\n",
    "print(x11.shape,x12.shape)\n",
    "samples1=int(x11.shape[0])\n",
    "samples2=int(x12.shape[0])\n",
    "features=x12.shape[1]\n",
    "print(features)\n",
    "\n",
    "#valores para inicializar el paso EM\n",
    "p=np.array([0.3,0.7])\n",
    "mu1=0.4*np.ones(features)\n",
    "mu2=0.3*np.ones(features)\n",
    "sigma1=np.identity(features)\n",
    "sigma2=np.identity(features)\n",
    "#sigma1=([0.1,0.2],[0.3,0.4])\n",
    "#sigma2=([0.4,0.5],[0.6,0.8])\n",
    "mu1,sigma1,mu2,sigma2,p,Nk1,Nk2=mixture.solve(clases,features,samples1,samples2,p,mu1,mu2,sigma1,sigma2,x11,x12)\n",
    "samples=x_train.shape[0]\n",
    "print('estps son los mu1 y mu2',mu1,mu2)\n",
    "y_hat,precision=mixture.get_prediction(x_train,mu1,sigma1,mu2,sigma2,features,samples,y_train)\n",
    "precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27940, 48) (27940, 48)\n",
      "[[-2.18886782 -0.31223998 -0.88368219 ...  0.61870714  0.63077574\n",
      "   0.61191785]\n",
      " [-2.18886782 -0.30716892 -0.84401342 ...  0.01973181  0.020036\n",
      "   0.01987001]\n",
      " [-2.18886782 -0.30716892 -0.84401342 ... -0.01239973 -0.01220737\n",
      "  -0.01238624]\n",
      " ...\n",
      " [ 0.09743488 -0.26491071 -0.39904913 ... -0.01050167 -0.01080887\n",
      "  -0.01080676]\n",
      " [ 0.75084546 -0.23797973 -0.85527783 ... -0.01293859 -0.01262878\n",
      "  -0.01334894]\n",
      " [-0.82954904 -0.30716892 -0.81618606 ... -0.01124189 -0.01095955\n",
      "  -0.01135704]]\n",
      "este es samples1 27940\n",
      "1\n",
      "esta es la gausiana [8.59820929e-24 2.23353285e-23 3.83186335e-24 6.84461873e-24\n",
      " 4.57453769e-24 6.92356506e-24 1.18715519e-24 1.55721063e-23\n",
      " 1.35123745e-27 1.02195373e-24] [1.75338098e-22 3.14519706e-28 2.58867977e-23 4.02574806e-26\n",
      " 2.01061327e-22 4.07960167e-23 1.54892641e-25 1.01526005e-23\n",
      " 2.54297188e-26 1.63650653e-23]\n",
      "este es el denominador [8.74476303e-24 8.56029234e-24 1.45352934e-24 2.53033364e-24\n",
      " 1.66061943e-24 2.56627506e-24 4.89874150e-25 6.12289401e-24\n",
      " 5.27562320e-28 3.62326793e-25]\n",
      "estos son los nk nan nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\52333\\Documents\\doctorado\\ml-md\\proyecto\\mixture.py:61: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  gama[i]=p*gausiana[i]/denominador[i]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan] [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]] [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan] [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]] [nan nan]\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\52333\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\linalg\\linalg.py:2146: RuntimeWarning: invalid value encountered in det\n",
      "  r = _umath_linalg.det(a, signature=signature)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esta es la gausiana [nan nan nan nan nan nan nan nan nan nan] [nan nan nan nan nan nan nan nan nan nan]\n",
      "este es el denominador [nan nan nan nan nan nan nan nan nan nan]\n",
      "estos son los nk nan nan\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan] [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]] [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan] [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]] [nan nan]\n",
      "3\n",
      "esta es la gausiana [nan nan nan nan nan nan nan nan nan nan] [nan nan nan nan nan nan nan nan nan nan]\n",
      "este es el denominador [nan nan nan nan nan nan nan nan nan nan]\n",
      "estos son los nk nan nan\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan] [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]] [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan] [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]] [nan nan]\n",
      "4\n",
      "esta es la gausiana [nan nan nan nan nan nan nan nan nan nan] [nan nan nan nan nan nan nan nan nan nan]\n",
      "este es el denominador [nan nan nan nan nan nan nan nan nan nan]\n",
      "estos son los nk nan nan\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan] [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]] [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan] [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]] [nan nan]\n",
      "5\n",
      "esta es la gausiana [nan nan nan nan nan nan nan nan nan nan] [nan nan nan nan nan nan nan nan nan nan]\n",
      "este es el denominador [nan nan nan nan nan nan nan nan nan nan]\n",
      "estos son los nk nan nan\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan] [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]] [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan] [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]] [nan nan]\n",
      "6\n",
      "esta es la gausiana [nan nan nan nan nan nan nan nan nan nan] [nan nan nan nan nan nan nan nan nan nan]\n",
      "este es el denominador [nan nan nan nan nan nan nan nan nan nan]\n",
      "estos son los nk nan nan\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan] [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]] [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan] [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]] [nan nan]\n",
      "7\n",
      "esta es la gausiana [nan nan nan nan nan nan nan nan nan nan] [nan nan nan nan nan nan nan nan nan nan]\n",
      "este es el denominador [nan nan nan nan nan nan nan nan nan nan]\n",
      "estos son los nk nan nan\n",
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan] [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]] [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan] [[nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " ...\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]\n",
      " [nan nan nan ... nan nan nan]] [nan nan]\n",
      "8\n",
      "esta es la gausiana [nan nan nan nan nan nan nan nan nan nan] [nan nan nan nan nan nan nan nan nan nan]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m sigma2\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39midentity(features)\n\u001b[0;32m     19\u001b[0m \u001b[39m#sigma1=([0.1,0.2],[0.3,0.4])\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m#sigma2=([0.4,0.5],[0.6,0.8])\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m mu1,sigma1,mu2,sigma2,p,Nk1,Nk2\u001b[39m=\u001b[39mmixture\u001b[39m.\u001b[39;49msolve(clases,features,samples1,samples2,p,mu1,mu2,sigma1,sigma2,x11,x12)\n\u001b[0;32m     22\u001b[0m samples\u001b[39m=\u001b[39mx_v\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m     23\u001b[0m y_hat,precision\u001b[39m=\u001b[39mmixture\u001b[39m.\u001b[39mget_prediction(x_v,mu1,sigma1,mu2,sigma2,features,samples,y_v)\n",
      "File \u001b[1;32mc:\\Users\\52333\\Documents\\doctorado\\ml-md\\proyecto\\mixture.py:135\u001b[0m, in \u001b[0;36msolve\u001b[1;34m(clases, features, samples1, samples2, p, mu1, mu2, sigma1, sigma2, x1, x2)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mesta es la gausiana\u001b[39m\u001b[39m'\u001b[39m,gausiana1[:\u001b[39m10\u001b[39m],gausiana2[:\u001b[39m10\u001b[39m])\n\u001b[0;32m    134\u001b[0m samples\u001b[39m=\u001b[39msamples1\u001b[39m+\u001b[39msamples2\n\u001b[1;32m--> 135\u001b[0m denominador\u001b[39m=\u001b[39mdenominador_gama(p,features,samples,sigma1,sigma2,mu1,mu2,x1,x2)\n\u001b[0;32m    136\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39meste es el denominador\u001b[39m\u001b[39m'\u001b[39m,denominador[:\u001b[39m10\u001b[39m])\n\u001b[0;32m    137\u001b[0m p1\u001b[39m=\u001b[39mp[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\52333\\Documents\\doctorado\\ml-md\\proyecto\\mixture.py:47\u001b[0m, in \u001b[0;36mdenominador_gama\u001b[1;34m(p, features, samples, sigma1, sigma2, mu1, mu2, x1, x2)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39m''' \u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[39msum_k(pi_k*N_k) es la misma para todos, sumamos sobre todas las clases\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     46\u001b[0m x\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mvstack((x1,x2))\n\u001b[1;32m---> 47\u001b[0m gausiana1\u001b[39m=\u001b[39mget_gaus(features,samples,sigma1,mu1,x)\n\u001b[0;32m     48\u001b[0m gausiana2\u001b[39m=\u001b[39mget_gaus(features,samples,sigma2,mu2,x)\n\u001b[0;32m     50\u001b[0m denominador\u001b[39m=\u001b[39mp[\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39mgausiana1\u001b[39m+\u001b[39mp[\u001b[39m1\u001b[39m]\u001b[39m*\u001b[39mgausiana2\n",
      "File \u001b[1;32mc:\\Users\\52333\\Documents\\doctorado\\ml-md\\proyecto\\mixture.py:28\u001b[0m, in \u001b[0;36mget_gaus\u001b[1;34m(features, samples, sigma, mu, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m inv_s\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39minv(sigma)\n\u001b[0;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(samples):\n\u001b[1;32m---> 28\u001b[0m     gausiana[i]\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39;49mtranspose(x[i,:]\u001b[39m-\u001b[39;49mmu)\u001b[39m@inv_s\u001b[39m\u001b[39m@\u001b[39m(x[i,:]\u001b[39m-\u001b[39mmu))\u001b[39m*\u001b[39m\u001b[39m1\u001b[39m\u001b[39m/\u001b[39m(np\u001b[39m.\u001b[39msqrt((\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mmt\u001b[39m.\u001b[39mpi)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfeatures\u001b[39m*\u001b[39mdet_s))\n\u001b[0;32m     29\u001b[0m \u001b[39mreturn\u001b[39;00m gausiana\n",
      "File \u001b[1;32m<__array_function__ internals>:177\u001b[0m, in \u001b[0;36mtranspose\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "clases=int(2)\n",
    "\n",
    "importlib.reload(mixture)\n",
    "x11=X[:27940]\n",
    "x12=X[27940:]\n",
    "print(x11.shape,x12.shape)\n",
    "samples1=int(x11.shape[0])\n",
    "samples2=int(x12.shape[0])\n",
    "features=x12.shape[1]\n",
    "print(x11)\n",
    "print('este es samples1',samples1)\n",
    "\n",
    "#valores para inicializar el paso EM\n",
    "p=np.array([0.3,0.7])\n",
    "mu1=0.3*np.ones(features)\n",
    "mu2=0.4*np.ones(features)\n",
    "sigma1=np.identity(features)\n",
    "sigma2=np.identity(features)\n",
    "#sigma1=([0.1,0.2],[0.3,0.4])\n",
    "#sigma2=([0.4,0.5],[0.6,0.8])\n",
    "mu1,sigma1,mu2,sigma2,p,Nk1,Nk2=mixture.solve(clases,features,samples1,samples2,p,mu1,mu2,sigma1,sigma2,x11,x12)\n",
    "samples=x_v.shape[0]\n",
    "y_hat,precision=mixture.get_prediction(x_v,mu1,sigma1,mu2,sigma2,features,samples,y_v)\n",
    "precision"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
